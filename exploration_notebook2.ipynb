{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "# , classification_report, confusion_matrix\n",
    "from models.net import WideOrthoResNet, OrthoVGG\n",
    "from layers.blocks import BasicBlock, HadamardBlock, HarmonicBlock, SlantBlock\n",
    "from loader import LoaderSmall\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import torch\n",
    "from torchvision.datasets import CIFAR10, CIFAR100, ImageNet\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "import gc\n",
    "from typing import List  # pylint: ignore\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from albumentations.augmentations import transforms as T\n",
    "import random\n",
    "import argparse\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "matplotlib.use('AGG')\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "weights=None\n",
    "dataset='skin'\n",
    "\n",
    "if dataset not in ['cifar10', 'cifar100', 'imagenet', 'isic2019']:\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                             (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    '''transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: F.pad(x.unsqueeze(0),\n",
    "                                      (4, 4, 4, 4),\n",
    "                                      mode='reflect').squeeze()),\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomCrop(64, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),'''\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                             (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    print(\"Loading metadata...\")\n",
    "    metadata = pd.read_csv('metadata/HAM10000_metadata.csv')\n",
    "    enc = LabelEncoder()\n",
    "    metadata['dx'] = enc.fit_transform(metadata['dx'])\n",
    "    metadata['dx_type'] = enc.fit_transform(metadata['dx_type'])\n",
    "    metadata['sex'] = enc.fit_transform(metadata['sex'])\n",
    "    metadata['localization'] = enc.fit_transform(metadata['localization'])\n",
    "    metadata['lesion_id'] = enc.fit_transform(metadata['lesion_id'])\n",
    "    labels = metadata.dx.values\n",
    "    imageid_path_dict = {x: f'./HAM10000_small/{x}.jpg' for x in metadata.image_id}\n",
    "\n",
    "    print(\"Loading Images...\")\n",
    "    train_names, val_names, \\\n",
    "    train_labels, val_labels = train_test_split(\n",
    "        np.asarray(list(imageid_path_dict.keys())),\n",
    "        labels,\n",
    "        test_size=0.15)\n",
    "    target = torch.from_numpy(train_labels)\n",
    "    class_sample_count = torch.tensor(\n",
    "        [(target == t).sum() for t in torch.unique(target, sorted=True)])\n",
    "    weight = 1. / class_sample_count.float()\n",
    "    samples_weight = torch.tensor([weight[t] for t in target]).float()\n",
    "    train_sampler = torch.utils \\\n",
    "        .data.WeightedRandomSampler(samples_weight,\n",
    "                                    len(samples_weight))\n",
    "    target = torch.from_numpy(val_labels)\n",
    "    class_sample_count = torch.tensor(\n",
    "        [(target == t).sum() for t in torch.unique(target, sorted=True)])\n",
    "    weight = 1. / class_sample_count.float()\n",
    "    samples_weight = torch.tensor([weight[t] for t in target]).float()\n",
    "    test_sampler = torch.utils \\\n",
    "        .data.WeightedRandomSampler(samples_weight,\n",
    "                                    len(samples_weight))\n",
    "    trainset = LoaderSmall(imageid_path_dict,\n",
    "                           labels = train_labels,\n",
    "                           names = train_names,\n",
    "                           weights=weights,\n",
    "                           weighting=False,\n",
    "                           transform=transform_train,\n",
    "                           color_space=None)\n",
    "    testset = LoaderSmall(imageid_path_dict,\n",
    "                          labels=val_labels,\n",
    "                          names=val_names,\n",
    "                          weighting=False,\n",
    "                          transform=transform_test,\n",
    "                          color_space=None)\n",
    "    \n",
    "    bs=32\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=bs,\n",
    "                                              sampler=train_sampler,\n",
    "                                              shuffle=False,\n",
    "                                              num_workers=2)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=bs,\n",
    "                                             shuffle=False,\n",
    "                                             sampler=test_sampler,\n",
    "                                             num_workers=2)\n",
    "    arch='wrn'\n",
    "    if arch == 'wrn':\n",
    "        net = WideOrthoResNet(in_channels=3,\n",
    "                              block=HadamardBlock,\n",
    "                              alpha_root=None,\n",
    "                              kernel_size=4,\n",
    "                              depth=22,\n",
    "                              widen_factor=3,\n",
    "                              num_classes=7,\n",
    "                              lmbda=None,\n",
    "                              diag=False)\n",
    "\n",
    "gc.collect()\n",
    "print(\"Number of trainable parameters:\", net._num_parameters()[0])\n",
    "\n",
    "net = net.cuda()\n",
    "net = torch.nn.DataParallel(\n",
    "    net,\n",
    "    device_ids=list(range(torch.cuda.device_count())))\n",
    "base_lr = 0.1\n",
    "best_acc = 0\n",
    "start_epoch = 0\n",
    "\n",
    "train_losses = []  # type: List[float]\n",
    "test_losses = []  # type: List[float]\n",
    "train_accs = []  # type: List[float]\n",
    "test_accs = []  # type: List[float]\n",
    "train_error = []\n",
    "test_error = []\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()#weight=torch.from_numpy(weights).cuda())\n",
    "\n",
    "optimizer = optim.SGD(params=net.parameters(),\n",
    "                      lr=base_lr,\n",
    "                      momentum=0.9,\n",
    "                      dampening=0,\n",
    "                      weight_decay=5e-4,\n",
    "                      nesterov=True)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "def get_lr(optimizer=optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "\n",
    "def one_hot_enc(output, target, num_classes=7):\n",
    "    labels = target.view((-1, 1))\n",
    "    batch_size, _ = labels.size()\n",
    "    labels_one_hot = torch.FloatTensor(\n",
    "        batch_size, num_classes).zero_().to('cuda')\n",
    "    labels_one_hot.scatter_(1, labels, 1)\n",
    "    return labels_one_hot\n",
    "\n",
    "\n",
    "# Training (https://github.com/kuangliu/pytorch-cifar/blob/master/main.py)\n",
    "def train(epoch, stop_after=1):\n",
    "    # print('\\nEpoch: %d' % epoch)\n",
    "    print(f\"\\nEpoch: {epoch}, learning rate = {lr:1.1e};\")\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(tqdm(trainloader)):\n",
    "        if stop_after is None or batch_idx<=stop_after:\n",
    "            inputs, targets = inputs.to('cuda'), targets.to('cuda')\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    print(\"Pred: \", predicted)\n",
    "    print(\"Truth: \", targets)\n",
    "    print(\n",
    "        'Loss: %.3f | Acc: %.3f%% (%d/%d)' % (train_loss / (len(trainloader)), 100. * correct / total, correct, total))\n",
    "    train_accs.append(100. * correct / total)\n",
    "    train_error.append(100. - 100. * correct / total)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "\n",
    "def test(epoch, stop_after=1):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(tqdm(testloader)):\n",
    "            if stop_after is None or batch_idx<=stop_after:\n",
    "                inputs, targets = inputs.to('cuda'), targets.to('cuda')\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "        print(\"Pred: \", predicted)\n",
    "        print(f\"Truth: {targets}\\n\")\n",
    "        print('Loss: %.3f | Acc: %.3f%% (%d/%d) | Error: %.3f%%' % (\n",
    "            test_loss / (len(testloader)), 100. * correct / total, correct, total, 100. * (1. - correct / total)))\n",
    "        test_accs.append(100. * correct / total)\n",
    "        test_error.append(100. - 100. * correct / total)\n",
    "        test_losses.append(test_losses)\n",
    "    # Save checkpoint.\n",
    "    acc = 100. * correct / total\n",
    "    if acc > best_acc:\n",
    "        '''print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "            'lr': get_lr(optimizer),\n",
    "            # 'optimizer': optimizer.state_dict()\n",
    "\n",
    "        }\n",
    "        if not os.path.isdir(f'checkpoint_{args.dataset}_{time.strftime(\"%Y_%m_%d\")}'):\n",
    "            os.mkdir(f'checkpoint_{args.dataset}_{time.strftime(\"%Y_%m_%d\")}')\n",
    "        a = ''\n",
    "        if args.alpha_root is not None:\n",
    "            a = f'alpha_{args.alpha_root}_'\n",
    "\n",
    "        torch.save(state,\n",
    "                   f'./checkpoint_{args.dataset}_{time.strftime(\"%Y_%m_%d\")}' +\n",
    "                   f'/ckpt_{args.block}_{args.arch}_{args.depth}_{args.widen}_{args.kernel_size}x{args.kernel_size}_' +\n",
    "                   a + f'_{acc:.2f}.t7')'''\n",
    "        best_acc = acc\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer,\n",
    "                         epoch,\n",
    "                         update_list=(25, 75),\n",
    "                         factor=10.,\n",
    "                         lim=1.):\n",
    "    # [60, 120, 160]  #[2,5,8,11,14,17,20]\n",
    "    if epoch in update_list:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = max(param_group['lr'] * factor, lim)\n",
    "    return\n",
    "\n",
    "\n",
    "def save_state(model, best_acc):\n",
    "    print('\\n==> Saving model ...\\n')\n",
    "    state = {'best_acc': best_acc,\n",
    "             'state_dict': model.state_dict()}\n",
    "    keys = list(state['state_dict'].keys())\n",
    "    for key in keys:\n",
    "        if 'module' in key:\n",
    "            state['state_dict'][key.replace('module.', '')] = \\\n",
    "                state['state_dict'].pop(key)\n",
    "    torch.save(state, 'harmonic_network.tar')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for epoch in range(start_epoch, 200):\n",
    "    adjust_learning_rate(optimizer, epoch, [60, 120, 160], factor=0.2, lim=1e-6)\n",
    "    lr = get_lr()\n",
    "    train(epoch,None)\n",
    "    test(epoch,0)\n",
    "    print(f\"Best Accuracy: {best_acc:.3f}\")\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_sample_count = torch.tensor(\n",
    "    [(trainset.labels == t).sum() for t in torch.unique(trainset.labels, sorted=True)])\n",
    "class_sample_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_sample_count = torch.tensor(\n",
    "    [(testset.labels == t).sum() for t in torch.unique(testset.labels, sorted=True)])\n",
    "class_sample_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv('metadata/HAM10000_metadata.csv')\n",
    "enc = LabelEncoder()\n",
    "metadata['dx'] = enc.fit_transform(metadata['dx'])\n",
    "metadata['dx_type'] = enc.fit_transform(metadata['dx_type'])\n",
    "metadata['sex'] = enc.fit_transform(metadata['sex'])\n",
    "metadata['localization'] = enc.fit_transform(metadata['localization'])\n",
    "metadata['lesion_id'] = enc.fit_transform(metadata['lesion_id'])\n",
    "labels = metadata.dx.values\n",
    "imageid_path_dict = {x: f'./HAM10000_small/{x}.jpg' for x in metadata.image_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.dx.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata['dx_type'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, batch = next(enumerate(testloader))\n",
    "labels = batch[1]\n",
    "torch.tensor(\n",
    "    [(labels == t).sum() for t in torch.unique(labels, sorted=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "exploration_notebook2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
