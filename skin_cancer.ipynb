{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "skin_cancer.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iliailmer/harmonic_network/blob/master/skin_cancer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "8e8-5D93OqU-",
        "colab_type": "code",
        "outputId": "2ce03bf0-6d97-45aa-e3eb-21f5337b226d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "cell_type": "code",
      "source": [
        "#!pip3 install fastai==0.7.0\n",
        "!apt-get -qq install -y libsm6 libxext6 && pip install -q -U opencv-python\n",
        "import cv2\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "#!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "#!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-1.0.0.post4-{platform}-linux_x86_64.whl torchvision\n",
        "!pip install -q torch==1.0.0 torchvision\n",
        "!pip install torch-dct\n",
        "\n",
        "import torch\n",
        "\n",
        "!pip install Pillow==4.0.0\n",
        "!pip install image\n",
        "!pip install torchtext==0.2.3\n",
        "\n",
        "!pip install kaggle\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "import io, os\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "drive_service = build('drive', 'v3')\n",
        "results = drive_service.files().list(\n",
        "        q=\"name = 'kaggle.json'\", fields=\"files(id)\").execute()\n",
        "kaggle_api_key = results.get('files', [])\n",
        "\n",
        "filename = \"/content/.kaggle/kaggle.json\"\n",
        "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "\n",
        "request = drive_service.files().get_media(fileId=kaggle_api_key[0]['id'])\n",
        "fh = io.FileIO(filename, 'wb')\n",
        "downloader = MediaIoBaseDownload(fh, request)\n",
        "done = False\n",
        "while done is False:\n",
        "    status, done = downloader.next_chunk()\n",
        "    print(\"Download %d%%.\" % int(status.progress() * 100))\n",
        "os.chmod(filename, 600)\n",
        "\n",
        "#!ln -s ~/.local/bin/kaggle /usr/bin/kaggle\n",
        "print(f\"CUDA is available: {torch.cuda.is_available()}\")\n",
        "torch.cuda.set_device(0)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp drive/My\\ Drive/kaggle.json ~/.kaggle/\n",
        "!kaggle datasets download -d kmader/skin-cancer-mnist-ham10000\n",
        "!unzip skin-cancer-mnist-ham10000.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading skin-cancer-mnist-ham10000.zip to /content\n",
            "100% 2.61G/2.62G [00:38<00:00, 87.2MB/s]\n",
            "100% 2.62G/2.62G [00:38<00:00, 72.4MB/s]\n",
            "Archive:  skin-cancer-mnist-ham10000.zip\n",
            "  inflating: hmnist_28_28_RGB.csv    \n",
            "  inflating: HAM10000_metadata.csv   \n",
            "  inflating: HAM10000_images_part_1.zip  \n",
            "  inflating: hmnist_28_28_L.csv      \n",
            "  inflating: hmnist_8_8_L.csv        \n",
            "  inflating: HAM10000_images_part_2.zip  \n",
            "  inflating: hmnist_8_8_RGB.csv      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "q9bGr0etR1aD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!mkdir HAM10000_images\n",
        "!unzip HAM10000_images_part_1.zip -d HAM10000_images\n",
        "!unzip HAM10000_images_part_2.zip -d HAM10000_images\n",
        "!rm HAM10000_images_part_1.zip HAM10000_images_part_2.zip skin-cancer-mnist-ham10000.zip\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3X5PzdFuSqI6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import fftpack\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import statistics\n",
        "import albumentations as alb\n",
        "import os\n",
        "\n",
        "from skimage import color, io, morphology, feature, segmentation, exposure\n",
        "from glob import glob\n",
        "import typing\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from skimage.exposure import rescale_intensity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3GCTgt5SRnRj",
        "colab_type": "code",
        "outputId": "af0d123d-4029-468c-b121-b3b44a9fc828",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "c1bUxJJiTJor",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def rescale(image, mn=0, mx=1):\n",
        "  return rescale_intensity(image,out_range=(mn,mx))\n",
        "\n",
        "def EME(image, window_height=11, window_width=11):\n",
        "    \"\"\"\n",
        "    EME measure for showing image quality based on human visual system.\n",
        "    For details, see\n",
        "    Agaian, Sos S., Karen Panetta,\n",
        "    and Artyom M. Grigoryan.\n",
        "    \"A new measure of image enhancement.\"\n",
        "    IASTED International Conference on Signal Processing & Communication.\n",
        "    Citeseer, 2000.\n",
        "\n",
        "    :param image: input image, must be single-channel.\n",
        "    :param window_height: height of the inspecting window\n",
        "    :param window_width: width of the inspecting window\n",
        "    :return: A real-valued enhancement measure.\n",
        "    \"\"\"\n",
        "    height, width = image.shape\n",
        "    # k_1 = np.floor(height/window_height)\n",
        "    # k_2 = np.floor(width/window_width)\n",
        "    sum_ = 0\n",
        "    k = 0\n",
        "    H = np.int(np.floor(window_height / 2))  # range in height, distance from the center of the window\n",
        "    W = np.int(np.floor(window_width / 2))  # range in width, same as above\n",
        "    for row in range(0 + H, height - H + 1, window_height):\n",
        "        for column in range(0 + W, width - W + 1, window_width):\n",
        "\n",
        "            window = image[row - H:row + H + 1, column - W:column + W + 1]\n",
        "\n",
        "            I_max = window.max()\n",
        "            I_min = window.min()\n",
        "\n",
        "            D = (I_max + 1) / (I_min + 1)\n",
        "            if D < 0.02:\n",
        "                D = 0.02\n",
        "            k += 1\n",
        "            sum_ += 20 * np.log(D)\n",
        "        # sum_k_1 += sum_k_2\n",
        "        # sum_k_2 = 0\n",
        "\n",
        "    eme = sum_ / k\n",
        "    return eme\n",
        "\n",
        "\n",
        "def EME_color(image):\n",
        "    \"\"\"\n",
        "    Application of EME to color images.\n",
        "    :param image: color image (multi-channel)\n",
        "    :return: EME of that image\n",
        "    \"\"\"\n",
        "    emes_ = []\n",
        "    if image.shape[-1]>1:\n",
        "        for each in range(image.shape[-1]):\n",
        "            emes_.append(EME(image[:,:,each]))\n",
        "    else:\n",
        "        emes_.append(EME(image))\n",
        "    return max(emes_)\n",
        "\n",
        "def alpha_rooting_fourier(image: np.ndarray, alpha: float = 0.9) -> np.ndarray:\n",
        "    ffted = fftpack.fft2(image)\n",
        "    abs_ffted = np.absolute(ffted)**alpha\n",
        "    iffted = fftpack.ifft2(abs_ffted*ffted/sp.absolute(ffted))\n",
        "    iffted = rescale(np.absolute(iffted), 0, 1)#.astype(int)\n",
        "    return iffted\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aDHXHgMB7xN5",
        "colab_type": "code",
        "outputId": "afae2400-3815-4685-cb45-29f66e8f98a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adc.json\t HAM10000_metadata.csv\thmnist_8_8_L.csv\n",
            "drive\t\t hmnist_28_28_L.csv\thmnist_8_8_RGB.csv\n",
            "HAM10000_images  hmnist_28_28_RGB.csv\tsample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Aqk6JVhiUCrn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lesion_type_dict = {\n",
        "    'nv': 'Melanocytic nevi',\n",
        "    'mel': 'Melanoma',\n",
        "    'bkl': 'Benign keratosis-like lesions ',\n",
        "    'bcc': 'Basal cell carcinoma',\n",
        "    'akiec': 'Actinic keratoses',\n",
        "    'vasc': 'Vascular lesions',\n",
        "    'df': 'Dermatofibroma'\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "metadata = pd.read_csv('HAM10000_metadata.csv')\n",
        "#imgs_28_rgb = pd.read_csv('hmnist_28_28_RGB.csv')\n",
        "\n",
        "#labels = imgs_28_rgb['label'].values\n",
        "\n",
        "#del imgs_28_rgb\n",
        "\n",
        "#diag_by_txt = dict(zip(list(dict(metadata.dx.value_counts()).keys()),\n",
        "#                       list(dict(pd.value_counts(labels)).keys())))\n",
        "#diag_by_label = dict(zip(list(dict(pd.value_counts(labels)).keys()), \n",
        "#                         list(dict(metadata.dx.value_counts()).keys())))\n",
        "\n",
        "#metadata['sex'] = pd.Categorical(metadata['sex']).codes\n",
        "#metadata['label'] = metadata['dx'].map(diag_by_txt.get)\n",
        "imageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x\n",
        "                     for x in glob(os.path.join('HAM10000_images', '*.jpg'))}\n",
        "#metadata['path'] = metadata['image_id'].map(imageid_path_dict.get)\n",
        "#metadata['image'] = metadata['path'].map(io.imread)\n",
        "#metadata['diag_name'] = metadata['dx'].map(lesion_type_dict.get) \n",
        "#metadata.head()\n",
        "#del metadata"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "34VJvTKZyahd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "4e454455-d90b-41aa-db1b-ab5b8867bfea"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import  LabelEncoder\n",
        "\n",
        "enc = LabelEncoder()\n",
        "metadata['dx'] =enc.fit_transform(metadata['dx'])\n",
        "metadata['dx_type'] =enc.fit_transform(metadata['dx_type'])\n",
        "metadata['sex'] =enc.fit_transform(metadata['sex'])\n",
        "metadata['localization'] =enc.fit_transform(metadata['localization'])\n",
        "metadata['lesion_id'] =enc.fit_transform(metadata['lesion_id'])\n",
        "metadata = metadata.sort_values(['image_id'], ascending=True).reset_index(drop=True)\n",
        "labels = metadata.dx.values\n",
        "metadata.head()\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lesion_id</th>\n",
              "      <th>image_id</th>\n",
              "      <th>dx</th>\n",
              "      <th>dx_type</th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>localization</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>550</td>\n",
              "      <td>ISIC_0024306</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>45.0</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3542</td>\n",
              "      <td>ISIC_0024307</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1471</td>\n",
              "      <td>ISIC_0024308</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>55.0</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>484</td>\n",
              "      <td>ISIC_0024309</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>40.0</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3320</td>\n",
              "      <td>ISIC_0024310</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   lesion_id      image_id  dx  dx_type   age  sex  localization\n",
              "0        550  ISIC_0024306   5        2  45.0    1            12\n",
              "1       3542  ISIC_0024307   5        2  50.0    1             9\n",
              "2       1471  ISIC_0024308   5        2  55.0    0            12\n",
              "3        484  ISIC_0024309   5        2  40.0    1            12\n",
              "4       3320  ISIC_0024310   4        3  60.0    1             3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "z58VTkL8Qwj_",
        "colab_type": "code",
        "outputId": "4d85df44-5aa0-4b83-bf40-adbcc52a74e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        }
      },
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-35e5c8a5ab93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'gc' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "eyBaQhnAp_2T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Flatten(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Flatten, self).__init__()\n",
        "    \n",
        "  def forward(self, input):\n",
        "    self.output_shape = input.view(input.size(0), -1).shape\n",
        "    return input.view(input.size(0), -1)\n",
        "      \n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from skimage import io\n",
        "from skimage.transform import resize\n",
        "from skimage import color\n",
        "import torchvision\n",
        "\n",
        "class Loader(Dataset):\n",
        "  def __init__(self,  path, labels, image_name = None,\n",
        "               train=True, transform = None, color_space = 'rgb'):\n",
        "    \"\"\"\n",
        "        Args:\n",
        "            image_name (string): Name of the file in the image\n",
        "            train (bool): whether to load training set or testing set\n",
        "            path (string): Path to the folder with images.\n",
        "            labels (np.ndarray): labels for images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "    \"\"\"\n",
        "    data = sorted(os.listdir(path))\n",
        "    self.path = path\n",
        "    self.train = train\n",
        "    self.name = image_name\n",
        "    self.transform = transform\n",
        "    self.train_names, self.test_names, self.train_labels,  self.test_labels = train_test_split(np.asarray(data), \n",
        "                                                                                              np.asarray(labels),\n",
        "                                                                                              test_size = 0.1)\n",
        "    self.color_transform_dict = {'rgb':color.rgb2rgbcie, 'hed':color.rgb2hed, 'hsv':color.rgb2hsv, None:None}\n",
        "    \n",
        "    if self.train:\n",
        "      if self.color_transform_dict[color_space] is not None:\n",
        "        self.train_data = torch.from_numpy(np.asarray([rescale(np.transpose(resize(self.color_transform_dict[color_space](io.imread(os.path.join(self.path, name))),(224,224),mode = 'reflect').astype('float32'), (2,1,0))) for name in self.train_names]))\n",
        "      else:\n",
        "        self.train_data = torch.from_numpy(np.asarray([rescale(np.transpose(resize(io.imread(os.path.join(self.path, name)),(224,224),mode = 'reflect').astype('float32'), (2,1,0))) for name in self.train_names]))\n",
        "      self.train_labels = torch.from_numpy(self.train_labels)\n",
        "    else:\n",
        "      if self.color_transform_dict[color_space] is not None:\n",
        "        self.test_data  = torch.from_numpy(np.asarray([rescale(np.transpose(resize(self.color_transform_dict[color_space](io.imread(os.path.join(self.path, name))),(224,224),mode = 'reflect').astype('float32'), (2,1,0))) for name in self.test_names]))\n",
        "      else:\n",
        "        self.test_data  = torch.from_numpy(np.asarray([rescale(np.transpose(resize(io.imread(os.path.join(self.path, name)),(224,224), mode = 'reflect').astype('float32'), (2,1,0))) for name in self.test_names]))\n",
        "      self.test_labels  = torch.from_numpy(self.test_labels)\n",
        "     \n",
        "      \n",
        "  def __len__(self):\n",
        "    if self.train:\n",
        "      return len(self.train_data) \n",
        "    else:\n",
        "      return len(self.test_data)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    if self.train:\n",
        "      image, label = self.train_data[index], self.train_labels[index]\n",
        "    else:\n",
        "      image, label = self.test_data[index], self.test_labels[index]\n",
        "    return image, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TFosALGnUYfz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class HarmonicBlock(nn.Module):\n",
        "  def __init__(self, input_channels, output_ch, bn=True,\n",
        "               kernel_size=4, lmbda = 3, pad = 0, stride = 1):\n",
        "    super(HarmonicBlock, self).__init__()\n",
        "    \"\"\"\n",
        "    :param input_channels: number of channels in the input\n",
        "    :param kernel_size: size of the kernel in the filter bank\n",
        "    :param pad: padding size\n",
        "    :param stride: stride size\n",
        "    :param lmbda: number of filters to be actually used (feature not implemented)\n",
        "    \"\"\"\n",
        "    self.bn = bn\n",
        "    self.input_channels = input_channels\n",
        "    self.output_ch = output_ch\n",
        "    self.pad = pad\n",
        "    self.stride = stride\n",
        "    self.K = kernel_size\n",
        "    self.N = self.K # preferably to have N=K !! (to fully replicate the paper), this is the convolution window size\n",
        "    self.PI = torch.as_tensor(np.pi)\n",
        "    self.lmbda = lmbda # limits the number of kernels\n",
        "    self.conv = nn.Conv2d(in_channels = self.K**2, out_channels = self.output_ch, \n",
        "                          kernel_size = 1, \n",
        "                          padding = 0,#self.pad,\n",
        "                          stride = 1) # output 1 because compresses into 1? (see formula 2)\n",
        "    self.get_filter_bank()\n",
        "    if self.bn:\n",
        "      self.bnorm = nn.BatchNorm2d(self.K**2)\n",
        "\n",
        "  \n",
        "  def fltr(self, u, v, N, k):\n",
        "    return torch.as_tensor([[torch.cos(torch.as_tensor(self.PI/N*(ii+0.5)*v))*torch.cos(torch.as_tensor(self.PI/N*(jj+0.5)*u)) for ii in range(k)] for jj in range(k)])\n",
        "  \n",
        "  \n",
        "  def get_filter_bank(self):\n",
        "    self.filter_bank = torch.stack([torch.stack([self.fltr(j, i, self.N, self.K) for i in range(self.K)]) for j in range(self.K)])\n",
        "    self.filter_bank = self.filter_bank.reshape([1,-1,self.K,self.K])\n",
        "    self.filter_bank = torch.cat([self.filter_bank]*self.input_channels, dim=0)\n",
        "    self.filter_bank = torch.transpose(self.filter_bank,0,1)#torch.cat([self.filter_bank]*self.input_channels, dim=1)\n",
        "    self.filter_bank = self.filter_bank.to('cuda') # without this, it does not get sent to cuda\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = F.conv2d(x, weight = self.filter_bank, padding=int(self.K/2), stride = self.stride)\n",
        "    if self.bn:\n",
        "      x = self.bnorm(x)\n",
        "    x = self.conv(x)      \n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1TN8qsQdb9hI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class HarmonicNet(nn.Module):\n",
        "  def __init__(self, in_ch,  out_ch, kernel_size,  stride, pad):\n",
        "    super(HarmonicNet, self).__init__()\n",
        "    self.input_channels = in_ch\n",
        "    self.kernel_size = kernel_size\n",
        "    self.stride = stride\n",
        "    self.pad = pad\n",
        "    \n",
        "    self.harmonic_block1 = HarmonicBlock(input_channels = self.input_channels,\n",
        "                                         output_ch = out_ch,\n",
        "                                         kernel_size = self.kernel_size, \n",
        "                                         pad = self.pad, \n",
        "                                         stride = self.stride)\n",
        "    self.pooling = nn.MaxPool2d(3,2)\n",
        "    self.harmonic_block2 = HarmonicBlock(input_channels = out_ch,\n",
        "                             output_ch = 64,\n",
        "                             kernel_size = 3, \n",
        "                             pad = 0, \n",
        "                             stride = 2)\n",
        "    self.harmonic_block3 = HarmonicBlock(input_channels = 64,\n",
        "                             output_ch = 128,\n",
        "                             kernel_size = 3, \n",
        "                             pad = 0, \n",
        "                             stride = 2)\n",
        "    self.flatten = Flatten()\n",
        "    self.linear1 = nn.Linear(128*28*28, 1024)\n",
        "    self.linear2 = nn.Linear(1024, 128)\n",
        "    self.linear3 = nn.Linear(128, 7)\n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.harmonic_block1(x)\n",
        "    x = self.harmonic_block2(x)\n",
        "    x = self.harmonic_block3(x)\n",
        "    x = self.flatten(x)\n",
        "    x = self.linear1(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.linear2(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.linear3(x)\n",
        "    x = x.view(-1, 7)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yqjy4Kwp4xCT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "outputId": "1430f8dc-d83e-4520-8ea9-6d5eea3e79cf"
      },
      "cell_type": "code",
      "source": [
        "x = trainloader.dataset.train_data[:2].to('cuda')\n",
        "print(x.shape)\n",
        "model_harmonic = HarmonicNet(3,18,3,2,1).cuda()#(3,32,4,2,0)\n",
        "x = model_harmonic.harmonic_block1(x)\n",
        "print(x.shape)\n",
        "x = model_harmonic.harmonic_block2(x)\n",
        "print(x.shape)\n",
        "x = model_harmonic.harmonic_block3(x)\n",
        "print(x.shape)\n",
        "plt.imshow(x[0].cpu().detach().numpy()[0])#model_harmonic.harmonic_block1.filter_bank[1].cpu().numpy()[0])#"
      ],
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 3, 224, 224])\n",
            "torch.Size([2, 18, 112, 112])\n",
            "torch.Size([2, 64, 56, 56])\n",
            "torch.Size([2, 128, 28, 28])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f9157e57320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 251
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f9157ed8eb8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "s9p8SoPpAJzj",
        "colab_type": "code",
        "outputId": "1f9fda78-e873-4825-98ee-c22bb257fcd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "model_harmonic = HarmonicNet(3,18,3,2,1)#(3,32,4,2,0)\n",
        "#model_harmonic\n",
        "\n",
        "for module in model_harmonic.modules():\n",
        "  if isinstance(module, nn.Conv2d):\n",
        "    module.weight.data.normal_(0,0.05)\n",
        "    module.bias.data.zero_()\n",
        "\n",
        "#model_harmonic = model_harmonic.to('cuda')\n",
        "#for parameter in model_harmonic.parameters():\n",
        "#  parameter.to('cuda')\n",
        "model_harmonic.cuda()\n",
        "model_harmonic = torch.nn.DataParallel(model_harmonic, device_ids=range(torch.cuda.device_count()))\n",
        "base_lr = float(1e-2)\n",
        "param_dict = dict(model_harmonic.named_parameters())\n",
        "params = []\n",
        "\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(model_harmonic.parameters(), lr=base_lr)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "best_acc=0\n",
        "\n",
        "gc.collect()\n"
      ],
      "execution_count": 274,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 274
        }
      ]
    },
    {
      "metadata": {
        "id": "NGIa8TOszdRV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#training_data_ = Loader(path = 'HAM10000_images', color_space='rgb',\n",
        "#           labels = labels, train=True, \n",
        "#           transform=torchvision.transforms.ToTensor())\n",
        "#gc.collect()\n",
        "#testing_data_ = Loader(path = 'HAM10000_images',color_space='rgb', \n",
        "#           labels = labels, train=False, \n",
        "#           transform=torchvision.transforms.ToTensor())\n",
        "#gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gKMZhyssCg7M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kPjCRD_zDU13",
        "colab_type": "code",
        "outputId": "a815d32d-3a4d-4758-921f-ecec3900ca2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "trainloader = torch.utils.data.DataLoader(training_data_, batch_size=64,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(testing_data_, batch_size = 64,\n",
        "                                         shuffle = True, num_workers = 2)\n",
        "classes = list(lesion_type_dict.keys())\n",
        "gc.collect()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "metadata": {
        "id": "srke62GASLqg",
        "colab_type": "code",
        "outputId": "a539f319-0f7d-41e6-f93d-cd320fd3ec22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "best_acc = 0\n",
        "def train(epoch, model, output_step=128):\n",
        "  model.train()\n",
        "  for batch_idx, (data, label) in enumerate(trainloader):\n",
        "    data, label = torch.autograd.Variable(data.cuda()), torch.autograd.Variable(label.cuda())\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    loss_ = loss(output, label)\n",
        "    _, y_pred = torch.max(output, 1)\n",
        "\n",
        "    \n",
        "    loss_.backward()\n",
        "    optimizer.step()\n",
        "    if batch_idx % output_step == 0:\n",
        "      print('\\nTrain Epoch: {} [{}/{} ({:.0f}%)]\\tTraining Loss: {:.6f}\\tLR: {}'.format(\n",
        "             epoch, batch_idx * len(data), len(trainloader.dataset),\n",
        "             100. * batch_idx / len(trainloader), loss_.data.item(),\n",
        "             optimizer.param_groups[0]['lr']))\n",
        "      train_losses.append(loss_.data.item())\n",
        "\n",
        "def test(epoch, model):\n",
        "  global best_acc\n",
        "  model.eval()\n",
        "  loss_ = 0\n",
        "  test_loss_ = 0\n",
        "  correct = 0\n",
        "  accuracy = 0\n",
        "  for batch_id, (data, label) in enumerate(testloader):\n",
        "    data, label = torch.autograd.Variable(data.cuda()), torch.autograd.Variable(label.cuda())\n",
        "    output = model(data)\n",
        "    test_loss_ = loss(output, label)\n",
        "    loss_ += loss(output, label).item()\n",
        "    pred = output.data.max(1, keepdim=True)[1]\n",
        "    correct += pred.eq(label.data.view_as(pred)).cpu().sum()\n",
        "    accuracy = 100. * correct / len(testloader.dataset)\n",
        "    \n",
        "  \n",
        "  if accuracy > best_acc:\n",
        "        best_acc = accuracy\n",
        "        save_state(model, best_acc)\n",
        "  print('Testing: {} [{}/{} ({:.0f}%)]\\tTesting Loss: {:.3e}\\tLR: {}'.format(\n",
        "             epoch, batch_id * len(data), len(testloader.dataset),\n",
        "             100. * batch_id / len(testloader), test_loss_.data.item(),\n",
        "             optimizer.param_groups[0]['lr']))\n",
        "  print(f'\\nTest set: Average loss: {loss_ * 128./len(testloader.dataset):.3f}, Accuracy: {correct}/{len(testloader.dataset):.3f}\\\n",
        "        ({100. * correct / len(testloader.dataset):.3f}%)')        \n",
        "  print(f'Best Accuracy: {best_acc:.2f}%\\n')\n",
        "  test_losses.append(test_loss_.data.item())\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    update_list = [20, 40, 80, 100, 160] #[120, 200, 240, 280]\n",
        "    if epoch in update_list:\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = param_group['lr'] * 0.5\n",
        "    return\n",
        "\n",
        "def save_state(model, best_acc):\n",
        "    print('\\n==> Saving model ...')\n",
        "    state = {\n",
        "            'best_acc': best_acc,\n",
        "            'state_dict': model.state_dict(),\n",
        "            }\n",
        "    keys = list(state['state_dict'].keys())\n",
        "    for key in keys:\n",
        "        if 'module' in key:\n",
        "            state['state_dict'][key.replace('module.', '')] = \\\n",
        "                    state['state_dict'].pop(key)\n",
        "    torch.save(state, 'harmonic_network.tar')\n",
        "\n",
        "gc.collect()"
      ],
      "execution_count": 275,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 275
        }
      ]
    },
    {
      "metadata": {
        "id": "kqYg_CXDESwI",
        "colab_type": "code",
        "outputId": "a2ce8514-7b53-4ab2-c21d-816c8c429bf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38918
        }
      },
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "for epoch in range(200):\n",
        "  adjust_learning_rate(optimizer, epoch)\n",
        "  train(epoch, model_harmonic, 64)\n",
        "  test(epoch, model_harmonic)"
      ],
      "execution_count": 276,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Epoch: 0 [0/9013 (0%)]\tTraining Loss: 1.950452\tLR: 0.01\n",
            "\n",
            "Train Epoch: 0 [4096/9013 (45%)]\tTraining Loss: 1.775601\tLR: 0.01\n",
            "\n",
            "Train Epoch: 0 [8192/9013 (91%)]\tTraining Loss: 1.580648\tLR: 0.01\n",
            "\n",
            "==> Saving model ...\n",
            "Testing: 0 [630/1002 (94%)]\tTesting Loss: 9.229e-01\tLR: 0.01\n",
            "\n",
            "Test set: Average loss: 1.979, Accuracy: 664/1002.000        (66.000%)\n",
            "Best Accuracy: 66.00%\n",
            "\n",
            "\n",
            "Train Epoch: 1 [0/9013 (0%)]\tTraining Loss: 0.908888\tLR: 0.01\n",
            "\n",
            "Train Epoch: 1 [4096/9013 (45%)]\tTraining Loss: 0.955292\tLR: 0.01\n",
            "\n",
            "Train Epoch: 1 [8192/9013 (91%)]\tTraining Loss: 0.916003\tLR: 0.01\n",
            "\n",
            "==> Saving model ...\n",
            "Testing: 1 [630/1002 (94%)]\tTesting Loss: 1.147e+00\tLR: 0.01\n",
            "\n",
            "Test set: Average loss: 1.726, Accuracy: 705/1002.000        (70.000%)\n",
            "Best Accuracy: 70.00%\n",
            "\n",
            "\n",
            "Train Epoch: 2 [0/9013 (0%)]\tTraining Loss: 0.641056\tLR: 0.01\n",
            "\n",
            "Train Epoch: 2 [4096/9013 (45%)]\tTraining Loss: 0.679200\tLR: 0.01\n",
            "\n",
            "Train Epoch: 2 [8192/9013 (91%)]\tTraining Loss: 0.929796\tLR: 0.01\n",
            "\n",
            "==> Saving model ...\n",
            "Testing: 2 [630/1002 (94%)]\tTesting Loss: 8.226e-01\tLR: 0.01\n",
            "\n",
            "Test set: Average loss: 1.550, Accuracy: 726/1002.000        (72.000%)\n",
            "Best Accuracy: 72.00%\n",
            "\n",
            "\n",
            "Train Epoch: 3 [0/9013 (0%)]\tTraining Loss: 0.776977\tLR: 0.01\n",
            "\n",
            "Train Epoch: 3 [4096/9013 (45%)]\tTraining Loss: 0.751766\tLR: 0.01\n",
            "\n",
            "Train Epoch: 3 [8192/9013 (91%)]\tTraining Loss: 0.840775\tLR: 0.01\n",
            "Testing: 3 [630/1002 (94%)]\tTesting Loss: 9.056e-01\tLR: 0.01\n",
            "\n",
            "Test set: Average loss: 1.598, Accuracy: 717/1002.000        (71.000%)\n",
            "Best Accuracy: 72.00%\n",
            "\n",
            "\n",
            "Train Epoch: 4 [0/9013 (0%)]\tTraining Loss: 0.669007\tLR: 0.01\n",
            "\n",
            "Train Epoch: 4 [4096/9013 (45%)]\tTraining Loss: 0.760691\tLR: 0.01\n",
            "\n",
            "Train Epoch: 4 [8192/9013 (91%)]\tTraining Loss: 0.848508\tLR: 0.01\n",
            "\n",
            "==> Saving model ...\n",
            "Testing: 4 [630/1002 (94%)]\tTesting Loss: 7.724e-01\tLR: 0.01\n",
            "\n",
            "Test set: Average loss: 1.370, Accuracy: 769/1002.000        (76.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 5 [0/9013 (0%)]\tTraining Loss: 0.751394\tLR: 0.01\n",
            "\n",
            "Train Epoch: 5 [4096/9013 (45%)]\tTraining Loss: 0.926723\tLR: 0.01\n",
            "\n",
            "Train Epoch: 5 [8192/9013 (91%)]\tTraining Loss: 0.856631\tLR: 0.01\n",
            "Testing: 5 [630/1002 (94%)]\tTesting Loss: 6.402e-01\tLR: 0.01\n",
            "\n",
            "Test set: Average loss: 1.336, Accuracy: 771/1002.000        (76.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 6 [0/9013 (0%)]\tTraining Loss: 0.596025\tLR: 0.01\n",
            "\n",
            "Train Epoch: 6 [4096/9013 (45%)]\tTraining Loss: 0.789813\tLR: 0.01\n",
            "\n",
            "Train Epoch: 6 [8192/9013 (91%)]\tTraining Loss: 0.918130\tLR: 0.01\n",
            "Testing: 6 [630/1002 (94%)]\tTesting Loss: 1.004e+00\tLR: 0.01\n",
            "\n",
            "Test set: Average loss: 1.695, Accuracy: 725/1002.000        (72.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 7 [0/9013 (0%)]\tTraining Loss: 1.029096\tLR: 0.01\n",
            "\n",
            "Train Epoch: 7 [4096/9013 (45%)]\tTraining Loss: 0.959014\tLR: 0.01\n",
            "\n",
            "Train Epoch: 7 [8192/9013 (91%)]\tTraining Loss: 0.844800\tLR: 0.01\n",
            "Testing: 7 [630/1002 (94%)]\tTesting Loss: 6.807e-01\tLR: 0.01\n",
            "\n",
            "Test set: Average loss: 1.754, Accuracy: 728/1002.000        (72.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 8 [0/9013 (0%)]\tTraining Loss: 0.870053\tLR: 0.01\n",
            "\n",
            "Train Epoch: 8 [4096/9013 (45%)]\tTraining Loss: 0.960092\tLR: 0.01\n",
            "\n",
            "Train Epoch: 8 [8192/9013 (91%)]\tTraining Loss: 50.946461\tLR: 0.01\n",
            "Testing: 8 [630/1002 (94%)]\tTesting Loss: 3.698e+03\tLR: 0.01\n",
            "\n",
            "Test set: Average loss: 8460.269, Accuracy: 564/1002.000        (56.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 9 [0/9013 (0%)]\tTraining Loss: 1754.309814\tLR: 0.01\n",
            "\n",
            "Train Epoch: 9 [4096/9013 (45%)]\tTraining Loss: 1744.293579\tLR: 0.01\n",
            "\n",
            "Train Epoch: 9 [8192/9013 (91%)]\tTraining Loss: 61.423462\tLR: 0.01\n",
            "Testing: 9 [630/1002 (94%)]\tTesting Loss: 4.318e+01\tLR: 0.01\n",
            "\n",
            "Test set: Average loss: 102.133, Accuracy: 654/1002.000        (65.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 10 [0/9013 (0%)]\tTraining Loss: 42.814796\tLR: 0.01\n",
            "\n",
            "Train Epoch: 10 [4096/9013 (45%)]\tTraining Loss: 14.259522\tLR: 0.01\n",
            "\n",
            "Train Epoch: 10 [8192/9013 (91%)]\tTraining Loss: 11.643867\tLR: 0.01\n",
            "Testing: 10 [630/1002 (94%)]\tTesting Loss: 1.171e+01\tLR: 0.01\n",
            "\n",
            "Test set: Average loss: 32.153, Accuracy: 511/1002.000        (50.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 11 [0/9013 (0%)]\tTraining Loss: 26.493359\tLR: 0.01\n",
            "\n",
            "Train Epoch: 11 [4096/9013 (45%)]\tTraining Loss: 21.498320\tLR: 0.01\n",
            "\n",
            "Train Epoch: 11 [8192/9013 (91%)]\tTraining Loss: 13.828955\tLR: 0.01\n",
            "Testing: 11 [630/1002 (94%)]\tTesting Loss: 1.857e+01\tLR: 0.01\n",
            "\n",
            "Test set: Average loss: 57.151, Accuracy: 651/1002.000        (64.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 12 [0/9013 (0%)]\tTraining Loss: 31.916975\tLR: 0.01\n",
            "\n",
            "Train Epoch: 12 [4096/9013 (45%)]\tTraining Loss: 17.902210\tLR: 0.01\n",
            "\n",
            "Train Epoch: 12 [8192/9013 (91%)]\tTraining Loss: 11.201582\tLR: 0.01\n",
            "Testing: 12 [630/1002 (94%)]\tTesting Loss: 9.314e+00\tLR: 0.01\n",
            "\n",
            "Test set: Average loss: 14.010, Accuracy: 620/1002.000        (61.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 13 [0/9013 (0%)]\tTraining Loss: 8.520814\tLR: 0.01\n",
            "\n",
            "Train Epoch: 13 [4096/9013 (45%)]\tTraining Loss: 10.695975\tLR: 0.01\n",
            "\n",
            "Train Epoch: 13 [8192/9013 (91%)]\tTraining Loss: 17.187498\tLR: 0.01\n",
            "Testing: 13 [630/1002 (94%)]\tTesting Loss: 1.697e+01\tLR: 0.01\n",
            "\n",
            "Test set: Average loss: 29.271, Accuracy: 633/1002.000        (63.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 14 [0/9013 (0%)]\tTraining Loss: 26.801514\tLR: 0.01\n",
            "\n",
            "Train Epoch: 14 [4096/9013 (45%)]\tTraining Loss: 11.676942\tLR: 0.01\n",
            "\n",
            "Train Epoch: 14 [8192/9013 (91%)]\tTraining Loss: 22.901129\tLR: 0.01\n",
            "Testing: 14 [630/1002 (94%)]\tTesting Loss: 1.081e+01\tLR: 0.01\n",
            "\n",
            "Test set: Average loss: 18.295, Accuracy: 653/1002.000        (65.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 15 [0/9013 (0%)]\tTraining Loss: 13.910433\tLR: 0.01\n",
            "\n",
            "Train Epoch: 15 [4096/9013 (45%)]\tTraining Loss: 5.908979\tLR: 0.01\n",
            "\n",
            "Train Epoch: 15 [8192/9013 (91%)]\tTraining Loss: 13.867411\tLR: 0.01\n",
            "Testing: 15 [630/1002 (94%)]\tTesting Loss: 7.147e+00\tLR: 0.01\n",
            "\n",
            "Test set: Average loss: 14.902, Accuracy: 652/1002.000        (65.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 16 [0/9013 (0%)]\tTraining Loss: 11.118513\tLR: 0.01\n",
            "\n",
            "Train Epoch: 16 [4096/9013 (45%)]\tTraining Loss: 5.467745\tLR: 0.01\n",
            "\n",
            "Train Epoch: 16 [8192/9013 (91%)]\tTraining Loss: 18.613321\tLR: 0.01\n",
            "Testing: 16 [630/1002 (94%)]\tTesting Loss: 2.901e+00\tLR: 0.01\n",
            "\n",
            "Test set: Average loss: 17.867, Accuracy: 647/1002.000        (64.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 17 [0/9013 (0%)]\tTraining Loss: 22.481815\tLR: 0.01\n",
            "\n",
            "Train Epoch: 17 [4096/9013 (45%)]\tTraining Loss: 8.406250\tLR: 0.01\n",
            "\n",
            "Train Epoch: 17 [8192/9013 (91%)]\tTraining Loss: 5.344140\tLR: 0.01\n",
            "Testing: 17 [630/1002 (94%)]\tTesting Loss: 5.498e+00\tLR: 0.01\n",
            "\n",
            "Test set: Average loss: 8.636, Accuracy: 663/1002.000        (66.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 18 [0/9013 (0%)]\tTraining Loss: 7.538095\tLR: 0.01\n",
            "\n",
            "Train Epoch: 18 [4096/9013 (45%)]\tTraining Loss: 7.694556\tLR: 0.01\n",
            "\n",
            "Train Epoch: 18 [8192/9013 (91%)]\tTraining Loss: 8.154807\tLR: 0.01\n",
            "Testing: 18 [630/1002 (94%)]\tTesting Loss: 4.835e+00\tLR: 0.01\n",
            "\n",
            "Test set: Average loss: 7.449, Accuracy: 664/1002.000        (66.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 19 [0/9013 (0%)]\tTraining Loss: 5.868614\tLR: 0.01\n",
            "\n",
            "Train Epoch: 19 [4096/9013 (45%)]\tTraining Loss: 8.456861\tLR: 0.01\n",
            "\n",
            "Train Epoch: 19 [8192/9013 (91%)]\tTraining Loss: 4.544178\tLR: 0.01\n",
            "Testing: 19 [630/1002 (94%)]\tTesting Loss: 4.670e+00\tLR: 0.01\n",
            "\n",
            "Test set: Average loss: 9.841, Accuracy: 663/1002.000        (66.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 20 [0/9013 (0%)]\tTraining Loss: 4.845078\tLR: 0.005\n",
            "\n",
            "Train Epoch: 20 [4096/9013 (45%)]\tTraining Loss: 2.327490\tLR: 0.005\n",
            "\n",
            "Train Epoch: 20 [8192/9013 (91%)]\tTraining Loss: 3.748480\tLR: 0.005\n",
            "Testing: 20 [630/1002 (94%)]\tTesting Loss: 2.640e+00\tLR: 0.005\n",
            "\n",
            "Test set: Average loss: 3.971, Accuracy: 668/1002.000        (66.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 21 [0/9013 (0%)]\tTraining Loss: 3.637377\tLR: 0.005\n",
            "\n",
            "Train Epoch: 21 [4096/9013 (45%)]\tTraining Loss: 2.006637\tLR: 0.005\n",
            "\n",
            "Train Epoch: 21 [8192/9013 (91%)]\tTraining Loss: 2.958926\tLR: 0.005\n",
            "Testing: 21 [630/1002 (94%)]\tTesting Loss: 1.369e+00\tLR: 0.005\n",
            "\n",
            "Test set: Average loss: 4.166, Accuracy: 663/1002.000        (66.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 22 [0/9013 (0%)]\tTraining Loss: 3.132172\tLR: 0.005\n",
            "\n",
            "Train Epoch: 22 [4096/9013 (45%)]\tTraining Loss: 3.348447\tLR: 0.005\n",
            "\n",
            "Train Epoch: 22 [8192/9013 (91%)]\tTraining Loss: 2.246251\tLR: 0.005\n",
            "Testing: 22 [630/1002 (94%)]\tTesting Loss: 3.434e+00\tLR: 0.005\n",
            "\n",
            "Test set: Average loss: 4.336, Accuracy: 670/1002.000        (66.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 23 [0/9013 (0%)]\tTraining Loss: 1.721272\tLR: 0.005\n",
            "\n",
            "Train Epoch: 23 [4096/9013 (45%)]\tTraining Loss: 4.472004\tLR: 0.005\n",
            "\n",
            "Train Epoch: 23 [8192/9013 (91%)]\tTraining Loss: 2.610895\tLR: 0.005\n",
            "Testing: 23 [630/1002 (94%)]\tTesting Loss: 2.547e+00\tLR: 0.005\n",
            "\n",
            "Test set: Average loss: 4.215, Accuracy: 668/1002.000        (66.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 24 [0/9013 (0%)]\tTraining Loss: 2.132060\tLR: 0.005\n",
            "\n",
            "Train Epoch: 24 [4096/9013 (45%)]\tTraining Loss: 4.038057\tLR: 0.005\n",
            "\n",
            "Train Epoch: 24 [8192/9013 (91%)]\tTraining Loss: 2.254143\tLR: 0.005\n",
            "Testing: 24 [630/1002 (94%)]\tTesting Loss: 1.750e+00\tLR: 0.005\n",
            "\n",
            "Test set: Average loss: 3.514, Accuracy: 688/1002.000        (68.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 25 [0/9013 (0%)]\tTraining Loss: 2.495538\tLR: 0.005\n",
            "\n",
            "Train Epoch: 25 [4096/9013 (45%)]\tTraining Loss: 1.504157\tLR: 0.005\n",
            "\n",
            "Train Epoch: 25 [8192/9013 (91%)]\tTraining Loss: 3.086087\tLR: 0.005\n",
            "Testing: 25 [630/1002 (94%)]\tTesting Loss: 1.160e+00\tLR: 0.005\n",
            "\n",
            "Test set: Average loss: 4.096, Accuracy: 682/1002.000        (68.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 26 [0/9013 (0%)]\tTraining Loss: 2.839177\tLR: 0.005\n",
            "\n",
            "Train Epoch: 26 [4096/9013 (45%)]\tTraining Loss: 3.336574\tLR: 0.005\n",
            "\n",
            "Train Epoch: 26 [8192/9013 (91%)]\tTraining Loss: 3.147638\tLR: 0.005\n",
            "Testing: 26 [630/1002 (94%)]\tTesting Loss: 1.751e+00\tLR: 0.005\n",
            "\n",
            "Test set: Average loss: 6.636, Accuracy: 667/1002.000        (66.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 27 [0/9013 (0%)]\tTraining Loss: 1.987223\tLR: 0.005\n",
            "\n",
            "Train Epoch: 27 [4096/9013 (45%)]\tTraining Loss: 1.613199\tLR: 0.005\n",
            "\n",
            "Train Epoch: 27 [8192/9013 (91%)]\tTraining Loss: 4.551526\tLR: 0.005\n",
            "Testing: 27 [630/1002 (94%)]\tTesting Loss: 1.867e+00\tLR: 0.005\n",
            "\n",
            "Test set: Average loss: 4.190, Accuracy: 667/1002.000        (66.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 28 [0/9013 (0%)]\tTraining Loss: 2.270646\tLR: 0.005\n",
            "\n",
            "Train Epoch: 28 [4096/9013 (45%)]\tTraining Loss: 2.737319\tLR: 0.005\n",
            "\n",
            "Train Epoch: 28 [8192/9013 (91%)]\tTraining Loss: 3.402476\tLR: 0.005\n",
            "Testing: 28 [630/1002 (94%)]\tTesting Loss: 1.857e+00\tLR: 0.005\n",
            "\n",
            "Test set: Average loss: 4.417, Accuracy: 674/1002.000        (67.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 29 [0/9013 (0%)]\tTraining Loss: 2.183869\tLR: 0.005\n",
            "\n",
            "Train Epoch: 29 [4096/9013 (45%)]\tTraining Loss: 2.257969\tLR: 0.005\n",
            "\n",
            "Train Epoch: 29 [8192/9013 (91%)]\tTraining Loss: 3.569274\tLR: 0.005\n",
            "Testing: 29 [630/1002 (94%)]\tTesting Loss: 1.455e+00\tLR: 0.005\n",
            "\n",
            "Test set: Average loss: 4.235, Accuracy: 680/1002.000        (67.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 30 [0/9013 (0%)]\tTraining Loss: 2.015303\tLR: 0.005\n",
            "\n",
            "Train Epoch: 30 [4096/9013 (45%)]\tTraining Loss: 3.493719\tLR: 0.005\n",
            "\n",
            "Train Epoch: 30 [8192/9013 (91%)]\tTraining Loss: 2.563421\tLR: 0.005\n",
            "Testing: 30 [630/1002 (94%)]\tTesting Loss: 1.780e+00\tLR: 0.005\n",
            "\n",
            "Test set: Average loss: 3.979, Accuracy: 678/1002.000        (67.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 31 [0/9013 (0%)]\tTraining Loss: 3.633379\tLR: 0.005\n",
            "\n",
            "Train Epoch: 31 [4096/9013 (45%)]\tTraining Loss: 3.026387\tLR: 0.005\n",
            "\n",
            "Train Epoch: 31 [8192/9013 (91%)]\tTraining Loss: 3.420542\tLR: 0.005\n",
            "Testing: 31 [630/1002 (94%)]\tTesting Loss: 1.303e+00\tLR: 0.005\n",
            "\n",
            "Test set: Average loss: 3.182, Accuracy: 686/1002.000        (68.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 32 [0/9013 (0%)]\tTraining Loss: 1.817863\tLR: 0.005\n",
            "\n",
            "Train Epoch: 32 [4096/9013 (45%)]\tTraining Loss: 2.461551\tLR: 0.005\n",
            "\n",
            "Train Epoch: 32 [8192/9013 (91%)]\tTraining Loss: 2.810818\tLR: 0.005\n",
            "Testing: 32 [630/1002 (94%)]\tTesting Loss: 2.134e+00\tLR: 0.005\n",
            "\n",
            "Test set: Average loss: 3.808, Accuracy: 695/1002.000        (69.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 33 [0/9013 (0%)]\tTraining Loss: 1.655266\tLR: 0.005\n",
            "\n",
            "Train Epoch: 33 [4096/9013 (45%)]\tTraining Loss: 3.137889\tLR: 0.005\n",
            "\n",
            "Train Epoch: 33 [8192/9013 (91%)]\tTraining Loss: 2.856458\tLR: 0.005\n",
            "Testing: 33 [630/1002 (94%)]\tTesting Loss: 3.504e+00\tLR: 0.005\n",
            "\n",
            "Test set: Average loss: 3.725, Accuracy: 707/1002.000        (70.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 34 [0/9013 (0%)]\tTraining Loss: 1.426141\tLR: 0.005\n",
            "\n",
            "Train Epoch: 34 [4096/9013 (45%)]\tTraining Loss: 1.855489\tLR: 0.005\n",
            "\n",
            "Train Epoch: 34 [8192/9013 (91%)]\tTraining Loss: 3.218256\tLR: 0.005\n",
            "Testing: 34 [630/1002 (94%)]\tTesting Loss: 1.733e+00\tLR: 0.005\n",
            "\n",
            "Test set: Average loss: 3.252, Accuracy: 707/1002.000        (70.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 35 [0/9013 (0%)]\tTraining Loss: 2.697603\tLR: 0.005\n",
            "\n",
            "Train Epoch: 35 [4096/9013 (45%)]\tTraining Loss: 2.886633\tLR: 0.005\n",
            "\n",
            "Train Epoch: 35 [8192/9013 (91%)]\tTraining Loss: 4.894523\tLR: 0.005\n",
            "Testing: 35 [630/1002 (94%)]\tTesting Loss: 1.708e+00\tLR: 0.005\n",
            "\n",
            "Test set: Average loss: 2.761, Accuracy: 700/1002.000        (69.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 36 [0/9013 (0%)]\tTraining Loss: 3.415276\tLR: 0.005\n",
            "\n",
            "Train Epoch: 36 [4096/9013 (45%)]\tTraining Loss: 1.768088\tLR: 0.005\n",
            "\n",
            "Train Epoch: 36 [8192/9013 (91%)]\tTraining Loss: 2.801331\tLR: 0.005\n",
            "Testing: 36 [630/1002 (94%)]\tTesting Loss: 1.579e+00\tLR: 0.005\n",
            "\n",
            "Test set: Average loss: 3.527, Accuracy: 705/1002.000        (70.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 37 [0/9013 (0%)]\tTraining Loss: 2.502899\tLR: 0.005\n",
            "\n",
            "Train Epoch: 37 [4096/9013 (45%)]\tTraining Loss: 1.910627\tLR: 0.005\n",
            "\n",
            "Train Epoch: 37 [8192/9013 (91%)]\tTraining Loss: 2.754898\tLR: 0.005\n",
            "Testing: 37 [630/1002 (94%)]\tTesting Loss: 2.219e+00\tLR: 0.005\n",
            "\n",
            "Test set: Average loss: 3.877, Accuracy: 706/1002.000        (70.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 38 [0/9013 (0%)]\tTraining Loss: 3.336349\tLR: 0.005\n",
            "\n",
            "Train Epoch: 38 [4096/9013 (45%)]\tTraining Loss: 3.836001\tLR: 0.005\n",
            "\n",
            "Train Epoch: 38 [8192/9013 (91%)]\tTraining Loss: 4.468997\tLR: 0.005\n",
            "Testing: 38 [630/1002 (94%)]\tTesting Loss: 1.541e+00\tLR: 0.005\n",
            "\n",
            "Test set: Average loss: 3.913, Accuracy: 704/1002.000        (70.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 39 [0/9013 (0%)]\tTraining Loss: 3.217080\tLR: 0.005\n",
            "\n",
            "Train Epoch: 39 [4096/9013 (45%)]\tTraining Loss: 0.910604\tLR: 0.005\n",
            "\n",
            "Train Epoch: 39 [8192/9013 (91%)]\tTraining Loss: 2.709128\tLR: 0.005\n",
            "Testing: 39 [630/1002 (94%)]\tTesting Loss: 1.168e+00\tLR: 0.005\n",
            "\n",
            "Test set: Average loss: 3.922, Accuracy: 702/1002.000        (70.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 40 [0/9013 (0%)]\tTraining Loss: 5.543944\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 40 [4096/9013 (45%)]\tTraining Loss: 1.217848\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 40 [8192/9013 (91%)]\tTraining Loss: 1.220190\tLR: 0.0025\n",
            "Testing: 40 [630/1002 (94%)]\tTesting Loss: 7.905e-01\tLR: 0.0025\n",
            "\n",
            "Test set: Average loss: 2.317, Accuracy: 723/1002.000        (72.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 41 [0/9013 (0%)]\tTraining Loss: 1.584298\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 41 [4096/9013 (45%)]\tTraining Loss: 1.368601\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 41 [8192/9013 (91%)]\tTraining Loss: 1.595776\tLR: 0.0025\n",
            "Testing: 41 [630/1002 (94%)]\tTesting Loss: 9.205e-01\tLR: 0.0025\n",
            "\n",
            "Test set: Average loss: 2.340, Accuracy: 700/1002.000        (69.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 42 [0/9013 (0%)]\tTraining Loss: 1.178526\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 42 [4096/9013 (45%)]\tTraining Loss: 1.508116\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 42 [8192/9013 (91%)]\tTraining Loss: 2.011990\tLR: 0.0025\n",
            "Testing: 42 [630/1002 (94%)]\tTesting Loss: 7.871e-01\tLR: 0.0025\n",
            "\n",
            "Test set: Average loss: 2.162, Accuracy: 712/1002.000        (71.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 43 [0/9013 (0%)]\tTraining Loss: 1.404898\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 43 [4096/9013 (45%)]\tTraining Loss: 1.048134\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 43 [8192/9013 (91%)]\tTraining Loss: 1.146655\tLR: 0.0025\n",
            "Testing: 43 [630/1002 (94%)]\tTesting Loss: 5.763e-01\tLR: 0.0025\n",
            "\n",
            "Test set: Average loss: 2.068, Accuracy: 719/1002.000        (71.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 44 [0/9013 (0%)]\tTraining Loss: 1.196647\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 44 [4096/9013 (45%)]\tTraining Loss: 1.242967\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 44 [8192/9013 (91%)]\tTraining Loss: 2.381888\tLR: 0.0025\n",
            "Testing: 44 [630/1002 (94%)]\tTesting Loss: 1.010e+00\tLR: 0.0025\n",
            "\n",
            "Test set: Average loss: 2.081, Accuracy: 726/1002.000        (72.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 45 [0/9013 (0%)]\tTraining Loss: 0.969220\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 45 [4096/9013 (45%)]\tTraining Loss: 1.160628\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 45 [8192/9013 (91%)]\tTraining Loss: 2.971632\tLR: 0.0025\n",
            "Testing: 45 [630/1002 (94%)]\tTesting Loss: 6.866e-01\tLR: 0.0025\n",
            "\n",
            "Test set: Average loss: 2.308, Accuracy: 718/1002.000        (71.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 46 [0/9013 (0%)]\tTraining Loss: 1.137386\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 46 [4096/9013 (45%)]\tTraining Loss: 2.144220\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 46 [8192/9013 (91%)]\tTraining Loss: 1.825012\tLR: 0.0025\n",
            "Testing: 46 [630/1002 (94%)]\tTesting Loss: 1.516e+00\tLR: 0.0025\n",
            "\n",
            "Test set: Average loss: 2.113, Accuracy: 730/1002.000        (72.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 47 [0/9013 (0%)]\tTraining Loss: 1.579370\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 47 [4096/9013 (45%)]\tTraining Loss: 1.819252\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 47 [8192/9013 (91%)]\tTraining Loss: 1.386049\tLR: 0.0025\n",
            "Testing: 47 [630/1002 (94%)]\tTesting Loss: 7.239e-01\tLR: 0.0025\n",
            "\n",
            "Test set: Average loss: 2.211, Accuracy: 742/1002.000        (74.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 48 [0/9013 (0%)]\tTraining Loss: 2.288344\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 48 [4096/9013 (45%)]\tTraining Loss: 2.040162\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 48 [8192/9013 (91%)]\tTraining Loss: 1.513397\tLR: 0.0025\n",
            "Testing: 48 [630/1002 (94%)]\tTesting Loss: 5.950e-01\tLR: 0.0025\n",
            "\n",
            "Test set: Average loss: 2.217, Accuracy: 749/1002.000        (74.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 49 [0/9013 (0%)]\tTraining Loss: 0.964615\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 49 [4096/9013 (45%)]\tTraining Loss: 1.256893\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 49 [8192/9013 (91%)]\tTraining Loss: 2.330689\tLR: 0.0025\n",
            "Testing: 49 [630/1002 (94%)]\tTesting Loss: 2.550e+00\tLR: 0.0025\n",
            "\n",
            "Test set: Average loss: 2.403, Accuracy: 730/1002.000        (72.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 50 [0/9013 (0%)]\tTraining Loss: 1.135079\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 50 [4096/9013 (45%)]\tTraining Loss: 2.164073\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 50 [8192/9013 (91%)]\tTraining Loss: 1.657081\tLR: 0.0025\n",
            "Testing: 50 [630/1002 (94%)]\tTesting Loss: 2.302e+00\tLR: 0.0025\n",
            "\n",
            "Test set: Average loss: 2.060, Accuracy: 758/1002.000        (75.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 51 [0/9013 (0%)]\tTraining Loss: 1.948703\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 51 [4096/9013 (45%)]\tTraining Loss: 1.932663\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 51 [8192/9013 (91%)]\tTraining Loss: 1.757598\tLR: 0.0025\n",
            "Testing: 51 [630/1002 (94%)]\tTesting Loss: 1.251e+00\tLR: 0.0025\n",
            "\n",
            "Test set: Average loss: 2.763, Accuracy: 730/1002.000        (72.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 52 [0/9013 (0%)]\tTraining Loss: 0.862496\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 52 [4096/9013 (45%)]\tTraining Loss: 1.125910\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 52 [8192/9013 (91%)]\tTraining Loss: 1.490547\tLR: 0.0025\n",
            "Testing: 52 [630/1002 (94%)]\tTesting Loss: 1.114e+00\tLR: 0.0025\n",
            "\n",
            "Test set: Average loss: 2.419, Accuracy: 727/1002.000        (72.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 53 [0/9013 (0%)]\tTraining Loss: 1.329098\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 53 [4096/9013 (45%)]\tTraining Loss: 1.074266\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 53 [8192/9013 (91%)]\tTraining Loss: 1.418932\tLR: 0.0025\n",
            "Testing: 53 [630/1002 (94%)]\tTesting Loss: 5.409e-01\tLR: 0.0025\n",
            "\n",
            "Test set: Average loss: 2.041, Accuracy: 744/1002.000        (74.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 54 [0/9013 (0%)]\tTraining Loss: 1.646226\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 54 [4096/9013 (45%)]\tTraining Loss: 1.002110\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 54 [8192/9013 (91%)]\tTraining Loss: 1.088892\tLR: 0.0025\n",
            "Testing: 54 [630/1002 (94%)]\tTesting Loss: 6.558e-01\tLR: 0.0025\n",
            "\n",
            "Test set: Average loss: 2.347, Accuracy: 732/1002.000        (73.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 55 [0/9013 (0%)]\tTraining Loss: 1.456763\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 55 [4096/9013 (45%)]\tTraining Loss: 1.637168\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 55 [8192/9013 (91%)]\tTraining Loss: 1.746966\tLR: 0.0025\n",
            "Testing: 55 [630/1002 (94%)]\tTesting Loss: 3.293e+00\tLR: 0.0025\n",
            "\n",
            "Test set: Average loss: 2.138, Accuracy: 739/1002.000        (73.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 56 [0/9013 (0%)]\tTraining Loss: 1.194029\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 56 [4096/9013 (45%)]\tTraining Loss: 1.268567\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 56 [8192/9013 (91%)]\tTraining Loss: 0.968948\tLR: 0.0025\n",
            "Testing: 56 [630/1002 (94%)]\tTesting Loss: 4.543e-01\tLR: 0.0025\n",
            "\n",
            "Test set: Average loss: 2.215, Accuracy: 753/1002.000        (75.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 57 [0/9013 (0%)]\tTraining Loss: 2.286924\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 57 [4096/9013 (45%)]\tTraining Loss: 1.189297\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 57 [8192/9013 (91%)]\tTraining Loss: 1.761761\tLR: 0.0025\n",
            "Testing: 57 [630/1002 (94%)]\tTesting Loss: 1.567e+00\tLR: 0.0025\n",
            "\n",
            "Test set: Average loss: 2.731, Accuracy: 710/1002.000        (70.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 58 [0/9013 (0%)]\tTraining Loss: 1.555761\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 58 [4096/9013 (45%)]\tTraining Loss: 1.451900\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 58 [8192/9013 (91%)]\tTraining Loss: 0.903918\tLR: 0.0025\n",
            "Testing: 58 [630/1002 (94%)]\tTesting Loss: 9.759e-01\tLR: 0.0025\n",
            "\n",
            "Test set: Average loss: 1.897, Accuracy: 763/1002.000        (76.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 59 [0/9013 (0%)]\tTraining Loss: 0.976555\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 59 [4096/9013 (45%)]\tTraining Loss: 1.064231\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 59 [8192/9013 (91%)]\tTraining Loss: 0.939000\tLR: 0.0025\n",
            "Testing: 59 [630/1002 (94%)]\tTesting Loss: 1.097e+00\tLR: 0.0025\n",
            "\n",
            "Test set: Average loss: 2.725, Accuracy: 733/1002.000        (73.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 60 [0/9013 (0%)]\tTraining Loss: 1.442569\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 60 [4096/9013 (45%)]\tTraining Loss: 1.391840\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 60 [8192/9013 (91%)]\tTraining Loss: 0.922819\tLR: 0.0025\n",
            "Testing: 60 [630/1002 (94%)]\tTesting Loss: 7.608e-01\tLR: 0.0025\n",
            "\n",
            "Test set: Average loss: 2.012, Accuracy: 758/1002.000        (75.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 61 [0/9013 (0%)]\tTraining Loss: 1.229296\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 61 [4096/9013 (45%)]\tTraining Loss: 1.410495\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 61 [8192/9013 (91%)]\tTraining Loss: 1.159696\tLR: 0.0025\n",
            "Testing: 61 [630/1002 (94%)]\tTesting Loss: 8.959e-01\tLR: 0.0025\n",
            "\n",
            "Test set: Average loss: 2.113, Accuracy: 760/1002.000        (75.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 62 [0/9013 (0%)]\tTraining Loss: 1.520071\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 62 [4096/9013 (45%)]\tTraining Loss: 0.986779\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 62 [8192/9013 (91%)]\tTraining Loss: 0.855354\tLR: 0.0025\n",
            "Testing: 62 [630/1002 (94%)]\tTesting Loss: 4.319e-01\tLR: 0.0025\n",
            "\n",
            "Test set: Average loss: 2.052, Accuracy: 760/1002.000        (75.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 63 [0/9013 (0%)]\tTraining Loss: 0.815193\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 63 [4096/9013 (45%)]\tTraining Loss: 0.874756\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 63 [8192/9013 (91%)]\tTraining Loss: 3.002808\tLR: 0.0025\n",
            "Testing: 63 [630/1002 (94%)]\tTesting Loss: 1.307e+00\tLR: 0.0025\n",
            "\n",
            "Test set: Average loss: 2.597, Accuracy: 718/1002.000        (71.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 64 [0/9013 (0%)]\tTraining Loss: 1.045739\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 64 [4096/9013 (45%)]\tTraining Loss: 0.694805\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 64 [8192/9013 (91%)]\tTraining Loss: 1.054663\tLR: 0.0025\n",
            "Testing: 64 [630/1002 (94%)]\tTesting Loss: 7.905e-01\tLR: 0.0025\n",
            "\n",
            "Test set: Average loss: 2.227, Accuracy: 760/1002.000        (75.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 65 [0/9013 (0%)]\tTraining Loss: 1.078546\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 65 [4096/9013 (45%)]\tTraining Loss: 1.053544\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 65 [8192/9013 (91%)]\tTraining Loss: 0.948274\tLR: 0.0025\n",
            "Testing: 65 [630/1002 (94%)]\tTesting Loss: 6.433e-01\tLR: 0.0025\n",
            "\n",
            "Test set: Average loss: 1.980, Accuracy: 752/1002.000        (75.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 66 [0/9013 (0%)]\tTraining Loss: 3.759046\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 66 [4096/9013 (45%)]\tTraining Loss: 0.806546\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 66 [8192/9013 (91%)]\tTraining Loss: 1.101914\tLR: 0.0025\n",
            "Testing: 66 [630/1002 (94%)]\tTesting Loss: 5.389e-01\tLR: 0.0025\n",
            "\n",
            "Test set: Average loss: 2.165, Accuracy: 750/1002.000        (74.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 67 [0/9013 (0%)]\tTraining Loss: 0.802437\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 67 [4096/9013 (45%)]\tTraining Loss: 1.116438\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 67 [8192/9013 (91%)]\tTraining Loss: 1.215487\tLR: 0.0025\n",
            "Testing: 67 [630/1002 (94%)]\tTesting Loss: 7.197e-01\tLR: 0.0025\n",
            "\n",
            "Test set: Average loss: 1.988, Accuracy: 771/1002.000        (76.000%)\n",
            "Best Accuracy: 76.00%\n",
            "\n",
            "\n",
            "Train Epoch: 68 [0/9013 (0%)]\tTraining Loss: 1.105624\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 68 [4096/9013 (45%)]\tTraining Loss: 0.571239\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 68 [8192/9013 (91%)]\tTraining Loss: 0.591490\tLR: 0.0025\n",
            "\n",
            "==> Saving model ...\n",
            "Testing: 68 [630/1002 (94%)]\tTesting Loss: 5.875e-01\tLR: 0.0025\n",
            "\n",
            "Test set: Average loss: 1.910, Accuracy: 772/1002.000        (77.000%)\n",
            "Best Accuracy: 77.00%\n",
            "\n",
            "\n",
            "Train Epoch: 69 [0/9013 (0%)]\tTraining Loss: 1.113091\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 69 [4096/9013 (45%)]\tTraining Loss: 1.952700\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 69 [8192/9013 (91%)]\tTraining Loss: 0.885187\tLR: 0.0025\n",
            "Testing: 69 [630/1002 (94%)]\tTesting Loss: 8.195e-01\tLR: 0.0025\n",
            "\n",
            "Test set: Average loss: 1.993, Accuracy: 769/1002.000        (76.000%)\n",
            "Best Accuracy: 77.00%\n",
            "\n",
            "\n",
            "Train Epoch: 70 [0/9013 (0%)]\tTraining Loss: 0.727838\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 70 [4096/9013 (45%)]\tTraining Loss: 0.454311\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 70 [8192/9013 (91%)]\tTraining Loss: 0.887848\tLR: 0.0025\n",
            "Testing: 70 [630/1002 (94%)]\tTesting Loss: 6.086e-01\tLR: 0.0025\n",
            "\n",
            "Test set: Average loss: 2.193, Accuracy: 776/1002.000        (77.000%)\n",
            "Best Accuracy: 77.00%\n",
            "\n",
            "\n",
            "Train Epoch: 71 [0/9013 (0%)]\tTraining Loss: 0.692732\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 71 [4096/9013 (45%)]\tTraining Loss: 0.773481\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 71 [8192/9013 (91%)]\tTraining Loss: 1.350327\tLR: 0.0025\n",
            "Testing: 71 [630/1002 (94%)]\tTesting Loss: 7.818e-01\tLR: 0.0025\n",
            "\n",
            "Test set: Average loss: 1.779, Accuracy: 771/1002.000        (76.000%)\n",
            "Best Accuracy: 77.00%\n",
            "\n",
            "\n",
            "Train Epoch: 72 [0/9013 (0%)]\tTraining Loss: 0.897017\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 72 [4096/9013 (45%)]\tTraining Loss: 0.871980\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 72 [8192/9013 (91%)]\tTraining Loss: 1.011976\tLR: 0.0025\n",
            "\n",
            "==> Saving model ...\n",
            "Testing: 72 [630/1002 (94%)]\tTesting Loss: 3.048e+00\tLR: 0.0025\n",
            "\n",
            "Test set: Average loss: 2.027, Accuracy: 789/1002.000        (78.000%)\n",
            "Best Accuracy: 78.00%\n",
            "\n",
            "\n",
            "Train Epoch: 73 [0/9013 (0%)]\tTraining Loss: 1.116910\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 73 [4096/9013 (45%)]\tTraining Loss: 1.437656\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 73 [8192/9013 (91%)]\tTraining Loss: 0.445106\tLR: 0.0025\n",
            "Testing: 73 [630/1002 (94%)]\tTesting Loss: 5.106e-01\tLR: 0.0025\n",
            "\n",
            "Test set: Average loss: 1.971, Accuracy: 763/1002.000        (76.000%)\n",
            "Best Accuracy: 78.00%\n",
            "\n",
            "\n",
            "Train Epoch: 74 [0/9013 (0%)]\tTraining Loss: 1.074482\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 74 [4096/9013 (45%)]\tTraining Loss: 0.920136\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 74 [8192/9013 (91%)]\tTraining Loss: 1.212220\tLR: 0.0025\n",
            "Testing: 74 [630/1002 (94%)]\tTesting Loss: 7.715e-01\tLR: 0.0025\n",
            "\n",
            "Test set: Average loss: 1.714, Accuracy: 778/1002.000        (77.000%)\n",
            "Best Accuracy: 78.00%\n",
            "\n",
            "\n",
            "Train Epoch: 75 [0/9013 (0%)]\tTraining Loss: 1.298754\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 75 [4096/9013 (45%)]\tTraining Loss: 1.001695\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 75 [8192/9013 (91%)]\tTraining Loss: 1.336950\tLR: 0.0025\n",
            "\n",
            "==> Saving model ...\n",
            "Testing: 75 [630/1002 (94%)]\tTesting Loss: 5.042e-01\tLR: 0.0025\n",
            "\n",
            "Test set: Average loss: 1.638, Accuracy: 800/1002.000        (79.000%)\n",
            "Best Accuracy: 79.00%\n",
            "\n",
            "\n",
            "Train Epoch: 76 [0/9013 (0%)]\tTraining Loss: 0.603962\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 76 [4096/9013 (45%)]\tTraining Loss: 0.759013\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 76 [8192/9013 (91%)]\tTraining Loss: 1.555778\tLR: 0.0025\n",
            "Testing: 76 [630/1002 (94%)]\tTesting Loss: 9.106e-01\tLR: 0.0025\n",
            "\n",
            "Test set: Average loss: 1.804, Accuracy: 782/1002.000        (78.000%)\n",
            "Best Accuracy: 79.00%\n",
            "\n",
            "\n",
            "Train Epoch: 77 [0/9013 (0%)]\tTraining Loss: 0.880013\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 77 [4096/9013 (45%)]\tTraining Loss: 0.765192\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 77 [8192/9013 (91%)]\tTraining Loss: 0.923984\tLR: 0.0025\n",
            "Testing: 77 [630/1002 (94%)]\tTesting Loss: 6.364e-01\tLR: 0.0025\n",
            "\n",
            "Test set: Average loss: 1.799, Accuracy: 768/1002.000        (76.000%)\n",
            "Best Accuracy: 79.00%\n",
            "\n",
            "\n",
            "Train Epoch: 78 [0/9013 (0%)]\tTraining Loss: 0.910155\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 78 [4096/9013 (45%)]\tTraining Loss: 0.974376\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 78 [8192/9013 (91%)]\tTraining Loss: 0.921562\tLR: 0.0025\n",
            "Testing: 78 [630/1002 (94%)]\tTesting Loss: 6.257e-01\tLR: 0.0025\n",
            "\n",
            "Test set: Average loss: 2.104, Accuracy: 764/1002.000        (76.000%)\n",
            "Best Accuracy: 79.00%\n",
            "\n",
            "\n",
            "Train Epoch: 79 [0/9013 (0%)]\tTraining Loss: 0.599520\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 79 [4096/9013 (45%)]\tTraining Loss: 1.720376\tLR: 0.0025\n",
            "\n",
            "Train Epoch: 79 [8192/9013 (91%)]\tTraining Loss: 0.704068\tLR: 0.0025\n",
            "Testing: 79 [630/1002 (94%)]\tTesting Loss: 4.254e-01\tLR: 0.0025\n",
            "\n",
            "Test set: Average loss: 1.716, Accuracy: 782/1002.000        (78.000%)\n",
            "Best Accuracy: 79.00%\n",
            "\n",
            "\n",
            "Train Epoch: 80 [0/9013 (0%)]\tTraining Loss: 0.633712\tLR: 0.00125\n",
            "\n",
            "Train Epoch: 80 [4096/9013 (45%)]\tTraining Loss: 0.562753\tLR: 0.00125\n",
            "\n",
            "Train Epoch: 80 [8192/9013 (91%)]\tTraining Loss: 0.521313\tLR: 0.00125\n",
            "\n",
            "==> Saving model ...\n",
            "Testing: 80 [630/1002 (94%)]\tTesting Loss: 7.721e-01\tLR: 0.00125\n",
            "\n",
            "Test set: Average loss: 1.489, Accuracy: 813/1002.000        (81.000%)\n",
            "Best Accuracy: 81.00%\n",
            "\n",
            "\n",
            "Train Epoch: 81 [0/9013 (0%)]\tTraining Loss: 0.665492\tLR: 0.00125\n",
            "\n",
            "Train Epoch: 81 [4096/9013 (45%)]\tTraining Loss: 0.391287\tLR: 0.00125\n",
            "\n",
            "Train Epoch: 81 [8192/9013 (91%)]\tTraining Loss: 0.680290\tLR: 0.00125\n",
            "\n",
            "==> Saving model ...\n",
            "Testing: 81 [630/1002 (94%)]\tTesting Loss: 7.335e-01\tLR: 0.00125\n",
            "\n",
            "Test set: Average loss: 1.401, Accuracy: 831/1002.000        (82.000%)\n",
            "Best Accuracy: 82.00%\n",
            "\n",
            "\n",
            "Train Epoch: 82 [0/9013 (0%)]\tTraining Loss: 0.328277\tLR: 0.00125\n",
            "\n",
            "Train Epoch: 82 [4096/9013 (45%)]\tTraining Loss: 0.281566\tLR: 0.00125\n",
            "\n",
            "Train Epoch: 82 [8192/9013 (91%)]\tTraining Loss: 0.413523\tLR: 0.00125\n",
            "\n",
            "==> Saving model ...\n",
            "Testing: 82 [630/1002 (94%)]\tTesting Loss: 8.775e-01\tLR: 0.00125\n",
            "\n",
            "Test set: Average loss: 1.349, Accuracy: 842/1002.000        (84.000%)\n",
            "Best Accuracy: 84.00%\n",
            "\n",
            "\n",
            "Train Epoch: 83 [0/9013 (0%)]\tTraining Loss: 0.338238\tLR: 0.00125\n",
            "\n",
            "Train Epoch: 83 [4096/9013 (45%)]\tTraining Loss: 0.488024\tLR: 0.00125\n",
            "\n",
            "Train Epoch: 83 [8192/9013 (91%)]\tTraining Loss: 0.409320\tLR: 0.00125\n",
            "Testing: 83 [630/1002 (94%)]\tTesting Loss: 1.888e+00\tLR: 0.00125\n",
            "\n",
            "Test set: Average loss: 1.411, Accuracy: 841/1002.000        (83.000%)\n",
            "Best Accuracy: 84.00%\n",
            "\n",
            "\n",
            "Train Epoch: 84 [0/9013 (0%)]\tTraining Loss: 0.488722\tLR: 0.00125\n",
            "\n",
            "Train Epoch: 84 [4096/9013 (45%)]\tTraining Loss: 0.424536\tLR: 0.00125\n",
            "\n",
            "Train Epoch: 84 [8192/9013 (91%)]\tTraining Loss: 0.502569\tLR: 0.00125\n",
            "Testing: 84 [630/1002 (94%)]\tTesting Loss: 6.172e-01\tLR: 0.00125\n",
            "\n",
            "Test set: Average loss: 1.443, Accuracy: 835/1002.000        (83.000%)\n",
            "Best Accuracy: 84.00%\n",
            "\n",
            "\n",
            "Train Epoch: 85 [0/9013 (0%)]\tTraining Loss: 0.616275\tLR: 0.00125\n",
            "\n",
            "Train Epoch: 85 [4096/9013 (45%)]\tTraining Loss: 0.587809\tLR: 0.00125\n",
            "\n",
            "Train Epoch: 85 [8192/9013 (91%)]\tTraining Loss: 0.534194\tLR: 0.00125\n",
            "Testing: 85 [630/1002 (94%)]\tTesting Loss: 6.631e-01\tLR: 0.00125\n",
            "\n",
            "Test set: Average loss: 1.425, Accuracy: 830/1002.000        (82.000%)\n",
            "Best Accuracy: 84.00%\n",
            "\n",
            "\n",
            "Train Epoch: 86 [0/9013 (0%)]\tTraining Loss: 0.657170\tLR: 0.00125\n",
            "\n",
            "Train Epoch: 86 [4096/9013 (45%)]\tTraining Loss: 0.654750\tLR: 0.00125\n",
            "\n",
            "Train Epoch: 86 [8192/9013 (91%)]\tTraining Loss: 0.355797\tLR: 0.00125\n",
            "Testing: 86 [630/1002 (94%)]\tTesting Loss: 3.602e-01\tLR: 0.00125\n",
            "\n",
            "Test set: Average loss: 1.392, Accuracy: 831/1002.000        (82.000%)\n",
            "Best Accuracy: 84.00%\n",
            "\n",
            "\n",
            "Train Epoch: 87 [0/9013 (0%)]\tTraining Loss: 0.469170\tLR: 0.00125\n",
            "\n",
            "Train Epoch: 87 [4096/9013 (45%)]\tTraining Loss: 0.555409\tLR: 0.00125\n",
            "\n",
            "Train Epoch: 87 [8192/9013 (91%)]\tTraining Loss: 0.874718\tLR: 0.00125\n",
            "Testing: 87 [630/1002 (94%)]\tTesting Loss: 6.560e-01\tLR: 0.00125\n",
            "\n",
            "Test set: Average loss: 1.383, Accuracy: 843/1002.000        (84.000%)\n",
            "Best Accuracy: 84.00%\n",
            "\n",
            "\n",
            "Train Epoch: 88 [0/9013 (0%)]\tTraining Loss: 0.412042\tLR: 0.00125\n",
            "\n",
            "Train Epoch: 88 [4096/9013 (45%)]\tTraining Loss: 0.327397\tLR: 0.00125\n",
            "\n",
            "Train Epoch: 88 [8192/9013 (91%)]\tTraining Loss: 0.587727\tLR: 0.00125\n",
            "Testing: 88 [630/1002 (94%)]\tTesting Loss: 1.279e+00\tLR: 0.00125\n",
            "\n",
            "Test set: Average loss: 1.450, Accuracy: 831/1002.000        (82.000%)\n",
            "Best Accuracy: 84.00%\n",
            "\n",
            "\n",
            "Train Epoch: 89 [0/9013 (0%)]\tTraining Loss: 0.484215\tLR: 0.00125\n",
            "\n",
            "Train Epoch: 89 [4096/9013 (45%)]\tTraining Loss: 0.536000\tLR: 0.00125\n",
            "\n",
            "Train Epoch: 89 [8192/9013 (91%)]\tTraining Loss: 0.694934\tLR: 0.00125\n",
            "Testing: 89 [630/1002 (94%)]\tTesting Loss: 1.749e-01\tLR: 0.00125\n",
            "\n",
            "Test set: Average loss: 1.284, Accuracy: 850/1002.000        (84.000%)\n",
            "Best Accuracy: 84.00%\n",
            "\n",
            "\n",
            "Train Epoch: 90 [0/9013 (0%)]\tTraining Loss: 0.343524\tLR: 0.00125\n",
            "\n",
            "Train Epoch: 90 [4096/9013 (45%)]\tTraining Loss: 0.500760\tLR: 0.00125\n",
            "\n",
            "Train Epoch: 90 [8192/9013 (91%)]\tTraining Loss: 0.340568\tLR: 0.00125\n",
            "\n",
            "==> Saving model ...\n",
            "Testing: 90 [630/1002 (94%)]\tTesting Loss: 1.226e-01\tLR: 0.00125\n",
            "\n",
            "Test set: Average loss: 1.352, Accuracy: 854/1002.000        (85.000%)\n",
            "Best Accuracy: 85.00%\n",
            "\n",
            "\n",
            "Train Epoch: 91 [0/9013 (0%)]\tTraining Loss: 0.205807\tLR: 0.00125\n",
            "\n",
            "Train Epoch: 91 [4096/9013 (45%)]\tTraining Loss: 0.479478\tLR: 0.00125\n",
            "\n",
            "Train Epoch: 91 [8192/9013 (91%)]\tTraining Loss: 0.487247\tLR: 0.00125\n",
            "Testing: 91 [630/1002 (94%)]\tTesting Loss: 1.022e+00\tLR: 0.00125\n",
            "\n",
            "Test set: Average loss: 1.459, Accuracy: 832/1002.000        (83.000%)\n",
            "Best Accuracy: 85.00%\n",
            "\n",
            "\n",
            "Train Epoch: 92 [0/9013 (0%)]\tTraining Loss: 0.518256\tLR: 0.00125\n",
            "\n",
            "Train Epoch: 92 [4096/9013 (45%)]\tTraining Loss: 0.469532\tLR: 0.00125\n",
            "\n",
            "Train Epoch: 92 [8192/9013 (91%)]\tTraining Loss: 0.524885\tLR: 0.00125\n",
            "Testing: 92 [630/1002 (94%)]\tTesting Loss: 1.284e+00\tLR: 0.00125\n",
            "\n",
            "Test set: Average loss: 1.288, Accuracy: 850/1002.000        (84.000%)\n",
            "Best Accuracy: 85.00%\n",
            "\n",
            "\n",
            "Train Epoch: 93 [0/9013 (0%)]\tTraining Loss: 0.319646\tLR: 0.00125\n",
            "\n",
            "Train Epoch: 93 [4096/9013 (45%)]\tTraining Loss: 0.534885\tLR: 0.00125\n",
            "\n",
            "Train Epoch: 93 [8192/9013 (91%)]\tTraining Loss: 0.465329\tLR: 0.00125\n",
            "Testing: 93 [630/1002 (94%)]\tTesting Loss: 4.310e-01\tLR: 0.00125\n",
            "\n",
            "Test set: Average loss: 1.415, Accuracy: 841/1002.000        (83.000%)\n",
            "Best Accuracy: 85.00%\n",
            "\n",
            "\n",
            "Train Epoch: 94 [0/9013 (0%)]\tTraining Loss: 0.387661\tLR: 0.00125\n",
            "\n",
            "Train Epoch: 94 [4096/9013 (45%)]\tTraining Loss: 0.302619\tLR: 0.00125\n",
            "\n",
            "Train Epoch: 94 [8192/9013 (91%)]\tTraining Loss: 0.216181\tLR: 0.00125\n",
            "Testing: 94 [630/1002 (94%)]\tTesting Loss: 3.739e-01\tLR: 0.00125\n",
            "\n",
            "Test set: Average loss: 1.418, Accuracy: 849/1002.000        (84.000%)\n",
            "Best Accuracy: 85.00%\n",
            "\n",
            "\n",
            "Train Epoch: 95 [0/9013 (0%)]\tTraining Loss: 0.767948\tLR: 0.00125\n",
            "\n",
            "Train Epoch: 95 [4096/9013 (45%)]\tTraining Loss: 0.170725\tLR: 0.00125\n",
            "\n",
            "Train Epoch: 95 [8192/9013 (91%)]\tTraining Loss: 0.374297\tLR: 0.00125\n",
            "Testing: 95 [630/1002 (94%)]\tTesting Loss: 3.977e-01\tLR: 0.00125\n",
            "\n",
            "Test set: Average loss: 1.336, Accuracy: 851/1002.000        (84.000%)\n",
            "Best Accuracy: 85.00%\n",
            "\n",
            "\n",
            "Train Epoch: 96 [0/9013 (0%)]\tTraining Loss: 0.458143\tLR: 0.00125\n",
            "\n",
            "Train Epoch: 96 [4096/9013 (45%)]\tTraining Loss: 0.541196\tLR: 0.00125\n",
            "\n",
            "Train Epoch: 96 [8192/9013 (91%)]\tTraining Loss: 0.267161\tLR: 0.00125\n",
            "Testing: 96 [630/1002 (94%)]\tTesting Loss: 4.674e-01\tLR: 0.00125\n",
            "\n",
            "Test set: Average loss: 1.339, Accuracy: 855/1002.000        (85.000%)\n",
            "Best Accuracy: 85.00%\n",
            "\n",
            "\n",
            "Train Epoch: 97 [0/9013 (0%)]\tTraining Loss: 0.456262\tLR: 0.00125\n",
            "\n",
            "Train Epoch: 97 [4096/9013 (45%)]\tTraining Loss: 0.456630\tLR: 0.00125\n",
            "\n",
            "Train Epoch: 97 [8192/9013 (91%)]\tTraining Loss: 0.317334\tLR: 0.00125\n",
            "\n",
            "==> Saving model ...\n",
            "Testing: 97 [630/1002 (94%)]\tTesting Loss: 7.589e-01\tLR: 0.00125\n",
            "\n",
            "Test set: Average loss: 1.322, Accuracy: 862/1002.000        (86.000%)\n",
            "Best Accuracy: 86.00%\n",
            "\n",
            "\n",
            "Train Epoch: 98 [0/9013 (0%)]\tTraining Loss: 0.299710\tLR: 0.00125\n",
            "\n",
            "Train Epoch: 98 [4096/9013 (45%)]\tTraining Loss: 0.532123\tLR: 0.00125\n",
            "\n",
            "Train Epoch: 98 [8192/9013 (91%)]\tTraining Loss: 0.786926\tLR: 0.00125\n",
            "Testing: 98 [630/1002 (94%)]\tTesting Loss: 6.536e-01\tLR: 0.00125\n",
            "\n",
            "Test set: Average loss: 1.390, Accuracy: 840/1002.000        (83.000%)\n",
            "Best Accuracy: 86.00%\n",
            "\n",
            "\n",
            "Train Epoch: 99 [0/9013 (0%)]\tTraining Loss: 0.819213\tLR: 0.00125\n",
            "\n",
            "Train Epoch: 99 [4096/9013 (45%)]\tTraining Loss: 0.644374\tLR: 0.00125\n",
            "\n",
            "Train Epoch: 99 [8192/9013 (91%)]\tTraining Loss: 0.635075\tLR: 0.00125\n",
            "Testing: 99 [630/1002 (94%)]\tTesting Loss: 4.122e-01\tLR: 0.00125\n",
            "\n",
            "Test set: Average loss: 1.257, Accuracy: 857/1002.000        (85.000%)\n",
            "Best Accuracy: 86.00%\n",
            "\n",
            "\n",
            "Train Epoch: 100 [0/9013 (0%)]\tTraining Loss: 0.210004\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 100 [4096/9013 (45%)]\tTraining Loss: 0.326858\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 100 [8192/9013 (91%)]\tTraining Loss: 0.316942\tLR: 0.000625\n",
            "\n",
            "==> Saving model ...\n",
            "Testing: 100 [630/1002 (94%)]\tTesting Loss: 6.347e-01\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.136, Accuracy: 891/1002.000        (88.000%)\n",
            "Best Accuracy: 88.00%\n",
            "\n",
            "\n",
            "Train Epoch: 101 [0/9013 (0%)]\tTraining Loss: 0.160058\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 101 [4096/9013 (45%)]\tTraining Loss: 0.398024\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 101 [8192/9013 (91%)]\tTraining Loss: 0.241521\tLR: 0.000625\n",
            "\n",
            "==> Saving model ...\n",
            "Testing: 101 [630/1002 (94%)]\tTesting Loss: 1.921e-01\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.192, Accuracy: 893/1002.000        (89.000%)\n",
            "Best Accuracy: 89.00%\n",
            "\n",
            "\n",
            "Train Epoch: 102 [0/9013 (0%)]\tTraining Loss: 0.131924\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 102 [4096/9013 (45%)]\tTraining Loss: 0.381665\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 102 [8192/9013 (91%)]\tTraining Loss: 0.153286\tLR: 0.000625\n",
            "Testing: 102 [630/1002 (94%)]\tTesting Loss: 1.116e-01\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.151, Accuracy: 897/1002.000        (89.000%)\n",
            "Best Accuracy: 89.00%\n",
            "\n",
            "\n",
            "Train Epoch: 103 [0/9013 (0%)]\tTraining Loss: 0.274400\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 103 [4096/9013 (45%)]\tTraining Loss: 0.175083\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 103 [8192/9013 (91%)]\tTraining Loss: 0.325946\tLR: 0.000625\n",
            "Testing: 103 [630/1002 (94%)]\tTesting Loss: 5.648e-01\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.252, Accuracy: 886/1002.000        (88.000%)\n",
            "Best Accuracy: 89.00%\n",
            "\n",
            "\n",
            "Train Epoch: 104 [0/9013 (0%)]\tTraining Loss: 0.312407\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 104 [4096/9013 (45%)]\tTraining Loss: 0.362463\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 104 [8192/9013 (91%)]\tTraining Loss: 0.227172\tLR: 0.000625\n",
            "Testing: 104 [630/1002 (94%)]\tTesting Loss: 3.929e-01\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.169, Accuracy: 894/1002.000        (89.000%)\n",
            "Best Accuracy: 89.00%\n",
            "\n",
            "\n",
            "Train Epoch: 105 [0/9013 (0%)]\tTraining Loss: 0.378920\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 105 [4096/9013 (45%)]\tTraining Loss: 0.215265\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 105 [8192/9013 (91%)]\tTraining Loss: 0.158840\tLR: 0.000625\n",
            "Testing: 105 [630/1002 (94%)]\tTesting Loss: 5.758e-01\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.197, Accuracy: 898/1002.000        (89.000%)\n",
            "Best Accuracy: 89.00%\n",
            "\n",
            "\n",
            "Train Epoch: 106 [0/9013 (0%)]\tTraining Loss: 0.397229\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 106 [4096/9013 (45%)]\tTraining Loss: 0.277151\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 106 [8192/9013 (91%)]\tTraining Loss: 0.315475\tLR: 0.000625\n",
            "\n",
            "==> Saving model ...\n",
            "Testing: 106 [630/1002 (94%)]\tTesting Loss: 1.462e-01\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.179, Accuracy: 902/1002.000        (90.000%)\n",
            "Best Accuracy: 90.00%\n",
            "\n",
            "\n",
            "Train Epoch: 107 [0/9013 (0%)]\tTraining Loss: 0.272334\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 107 [4096/9013 (45%)]\tTraining Loss: 0.476499\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 107 [8192/9013 (91%)]\tTraining Loss: 0.495534\tLR: 0.000625\n",
            "Testing: 107 [630/1002 (94%)]\tTesting Loss: 8.963e-01\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.268, Accuracy: 895/1002.000        (89.000%)\n",
            "Best Accuracy: 90.00%\n",
            "\n",
            "\n",
            "Train Epoch: 108 [0/9013 (0%)]\tTraining Loss: 0.198733\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 108 [4096/9013 (45%)]\tTraining Loss: 0.239781\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 108 [8192/9013 (91%)]\tTraining Loss: 0.230586\tLR: 0.000625\n",
            "Testing: 108 [630/1002 (94%)]\tTesting Loss: 7.330e-01\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.173, Accuracy: 902/1002.000        (90.000%)\n",
            "Best Accuracy: 90.00%\n",
            "\n",
            "\n",
            "Train Epoch: 109 [0/9013 (0%)]\tTraining Loss: 0.205197\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 109 [4096/9013 (45%)]\tTraining Loss: 0.217108\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 109 [8192/9013 (91%)]\tTraining Loss: 0.315629\tLR: 0.000625\n",
            "Testing: 109 [630/1002 (94%)]\tTesting Loss: 2.825e-01\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.287, Accuracy: 901/1002.000        (89.000%)\n",
            "Best Accuracy: 90.00%\n",
            "\n",
            "\n",
            "Train Epoch: 110 [0/9013 (0%)]\tTraining Loss: 0.219255\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 110 [4096/9013 (45%)]\tTraining Loss: 0.476811\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 110 [8192/9013 (91%)]\tTraining Loss: 0.174245\tLR: 0.000625\n",
            "\n",
            "==> Saving model ...\n",
            "Testing: 110 [630/1002 (94%)]\tTesting Loss: 7.092e-01\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.258, Accuracy: 913/1002.000        (91.000%)\n",
            "Best Accuracy: 91.00%\n",
            "\n",
            "\n",
            "Train Epoch: 111 [0/9013 (0%)]\tTraining Loss: 0.216083\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 111 [4096/9013 (45%)]\tTraining Loss: 0.336358\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 111 [8192/9013 (91%)]\tTraining Loss: 0.318404\tLR: 0.000625\n",
            "Testing: 111 [630/1002 (94%)]\tTesting Loss: 9.842e-01\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.391, Accuracy: 896/1002.000        (89.000%)\n",
            "Best Accuracy: 91.00%\n",
            "\n",
            "\n",
            "Train Epoch: 112 [0/9013 (0%)]\tTraining Loss: 0.135604\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 112 [4096/9013 (45%)]\tTraining Loss: 0.192818\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 112 [8192/9013 (91%)]\tTraining Loss: 0.309033\tLR: 0.000625\n",
            "Testing: 112 [630/1002 (94%)]\tTesting Loss: 9.178e-01\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.216, Accuracy: 909/1002.000        (90.000%)\n",
            "Best Accuracy: 91.00%\n",
            "\n",
            "\n",
            "Train Epoch: 113 [0/9013 (0%)]\tTraining Loss: 0.166488\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 113 [4096/9013 (45%)]\tTraining Loss: 0.179485\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 113 [8192/9013 (91%)]\tTraining Loss: 0.204757\tLR: 0.000625\n",
            "Testing: 113 [630/1002 (94%)]\tTesting Loss: 1.442e+00\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.282, Accuracy: 909/1002.000        (90.000%)\n",
            "Best Accuracy: 91.00%\n",
            "\n",
            "\n",
            "Train Epoch: 114 [0/9013 (0%)]\tTraining Loss: 0.130794\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 114 [4096/9013 (45%)]\tTraining Loss: 0.383528\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 114 [8192/9013 (91%)]\tTraining Loss: 0.241988\tLR: 0.000625\n",
            "Testing: 114 [630/1002 (94%)]\tTesting Loss: 4.267e-01\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.240, Accuracy: 906/1002.000        (90.000%)\n",
            "Best Accuracy: 91.00%\n",
            "\n",
            "\n",
            "Train Epoch: 115 [0/9013 (0%)]\tTraining Loss: 0.228359\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 115 [4096/9013 (45%)]\tTraining Loss: 0.285206\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 115 [8192/9013 (91%)]\tTraining Loss: 0.275018\tLR: 0.000625\n",
            "Testing: 115 [630/1002 (94%)]\tTesting Loss: 1.233e+00\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.315, Accuracy: 903/1002.000        (90.000%)\n",
            "Best Accuracy: 91.00%\n",
            "\n",
            "\n",
            "Train Epoch: 116 [0/9013 (0%)]\tTraining Loss: 0.393033\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 116 [4096/9013 (45%)]\tTraining Loss: 0.310727\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 116 [8192/9013 (91%)]\tTraining Loss: 0.274276\tLR: 0.000625\n",
            "Testing: 116 [630/1002 (94%)]\tTesting Loss: 7.867e-01\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.296, Accuracy: 900/1002.000        (89.000%)\n",
            "Best Accuracy: 91.00%\n",
            "\n",
            "\n",
            "Train Epoch: 117 [0/9013 (0%)]\tTraining Loss: 0.153556\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 117 [4096/9013 (45%)]\tTraining Loss: 0.158995\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 117 [8192/9013 (91%)]\tTraining Loss: 0.193397\tLR: 0.000625\n",
            "Testing: 117 [630/1002 (94%)]\tTesting Loss: 2.282e-01\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.346, Accuracy: 899/1002.000        (89.000%)\n",
            "Best Accuracy: 91.00%\n",
            "\n",
            "\n",
            "Train Epoch: 118 [0/9013 (0%)]\tTraining Loss: 0.333904\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 118 [4096/9013 (45%)]\tTraining Loss: 0.212182\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 118 [8192/9013 (91%)]\tTraining Loss: 0.233047\tLR: 0.000625\n",
            "Testing: 118 [630/1002 (94%)]\tTesting Loss: 4.019e-02\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.327, Accuracy: 906/1002.000        (90.000%)\n",
            "Best Accuracy: 91.00%\n",
            "\n",
            "\n",
            "Train Epoch: 119 [0/9013 (0%)]\tTraining Loss: 0.278378\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 119 [4096/9013 (45%)]\tTraining Loss: 0.158217\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 119 [8192/9013 (91%)]\tTraining Loss: 0.266834\tLR: 0.000625\n",
            "Testing: 119 [630/1002 (94%)]\tTesting Loss: 2.844e-01\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.319, Accuracy: 913/1002.000        (91.000%)\n",
            "Best Accuracy: 91.00%\n",
            "\n",
            "\n",
            "Train Epoch: 120 [0/9013 (0%)]\tTraining Loss: 0.204209\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 120 [4096/9013 (45%)]\tTraining Loss: 0.194037\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 120 [8192/9013 (91%)]\tTraining Loss: 0.415477\tLR: 0.000625\n",
            "Testing: 120 [630/1002 (94%)]\tTesting Loss: 1.861e+00\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.391, Accuracy: 918/1002.000        (91.000%)\n",
            "Best Accuracy: 91.00%\n",
            "\n",
            "\n",
            "Train Epoch: 121 [0/9013 (0%)]\tTraining Loss: 0.321111\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 121 [4096/9013 (45%)]\tTraining Loss: 0.149910\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 121 [8192/9013 (91%)]\tTraining Loss: 0.319877\tLR: 0.000625\n",
            "\n",
            "==> Saving model ...\n",
            "Testing: 121 [630/1002 (94%)]\tTesting Loss: 1.623e+00\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.386, Accuracy: 923/1002.000        (92.000%)\n",
            "Best Accuracy: 92.00%\n",
            "\n",
            "\n",
            "Train Epoch: 122 [0/9013 (0%)]\tTraining Loss: 0.137225\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 122 [4096/9013 (45%)]\tTraining Loss: 0.322426\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 122 [8192/9013 (91%)]\tTraining Loss: 0.163997\tLR: 0.000625\n",
            "Testing: 122 [630/1002 (94%)]\tTesting Loss: 2.822e-01\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.296, Accuracy: 920/1002.000        (91.000%)\n",
            "Best Accuracy: 92.00%\n",
            "\n",
            "\n",
            "Train Epoch: 123 [0/9013 (0%)]\tTraining Loss: 0.258522\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 123 [4096/9013 (45%)]\tTraining Loss: 0.197694\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 123 [8192/9013 (91%)]\tTraining Loss: 0.199747\tLR: 0.000625\n",
            "Testing: 123 [630/1002 (94%)]\tTesting Loss: 1.784e-01\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.285, Accuracy: 926/1002.000        (92.000%)\n",
            "Best Accuracy: 92.00%\n",
            "\n",
            "\n",
            "Train Epoch: 124 [0/9013 (0%)]\tTraining Loss: 0.329471\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 124 [4096/9013 (45%)]\tTraining Loss: 0.105193\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 124 [8192/9013 (91%)]\tTraining Loss: 0.257473\tLR: 0.000625\n",
            "Testing: 124 [630/1002 (94%)]\tTesting Loss: 2.395e-01\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.354, Accuracy: 918/1002.000        (91.000%)\n",
            "Best Accuracy: 92.00%\n",
            "\n",
            "\n",
            "Train Epoch: 125 [0/9013 (0%)]\tTraining Loss: 0.248131\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 125 [4096/9013 (45%)]\tTraining Loss: 0.282009\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 125 [8192/9013 (91%)]\tTraining Loss: 0.675452\tLR: 0.000625\n",
            "Testing: 125 [630/1002 (94%)]\tTesting Loss: 3.591e-01\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.442, Accuracy: 909/1002.000        (90.000%)\n",
            "Best Accuracy: 92.00%\n",
            "\n",
            "\n",
            "Train Epoch: 126 [0/9013 (0%)]\tTraining Loss: 0.231852\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 126 [4096/9013 (45%)]\tTraining Loss: 0.152444\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 126 [8192/9013 (91%)]\tTraining Loss: 0.160102\tLR: 0.000625\n",
            "Testing: 126 [630/1002 (94%)]\tTesting Loss: 1.279e+00\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.499, Accuracy: 926/1002.000        (92.000%)\n",
            "Best Accuracy: 92.00%\n",
            "\n",
            "\n",
            "Train Epoch: 127 [0/9013 (0%)]\tTraining Loss: 0.161823\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 127 [4096/9013 (45%)]\tTraining Loss: 0.203369\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 127 [8192/9013 (91%)]\tTraining Loss: 0.090779\tLR: 0.000625\n",
            "Testing: 127 [630/1002 (94%)]\tTesting Loss: 1.985e+00\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.509, Accuracy: 920/1002.000        (91.000%)\n",
            "Best Accuracy: 92.00%\n",
            "\n",
            "\n",
            "Train Epoch: 128 [0/9013 (0%)]\tTraining Loss: 0.301080\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 128 [4096/9013 (45%)]\tTraining Loss: 0.302368\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 128 [8192/9013 (91%)]\tTraining Loss: 0.167479\tLR: 0.000625\n",
            "Testing: 128 [630/1002 (94%)]\tTesting Loss: 2.025e+00\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.504, Accuracy: 925/1002.000        (92.000%)\n",
            "Best Accuracy: 92.00%\n",
            "\n",
            "\n",
            "Train Epoch: 129 [0/9013 (0%)]\tTraining Loss: 0.186969\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 129 [4096/9013 (45%)]\tTraining Loss: 0.113470\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 129 [8192/9013 (91%)]\tTraining Loss: 0.251696\tLR: 0.000625\n",
            "Testing: 129 [630/1002 (94%)]\tTesting Loss: 1.279e-01\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.423, Accuracy: 917/1002.000        (91.000%)\n",
            "Best Accuracy: 92.00%\n",
            "\n",
            "\n",
            "Train Epoch: 130 [0/9013 (0%)]\tTraining Loss: 0.142978\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 130 [4096/9013 (45%)]\tTraining Loss: 0.259173\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 130 [8192/9013 (91%)]\tTraining Loss: 0.284724\tLR: 0.000625\n",
            "Testing: 130 [630/1002 (94%)]\tTesting Loss: 3.407e-01\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.423, Accuracy: 924/1002.000        (92.000%)\n",
            "Best Accuracy: 92.00%\n",
            "\n",
            "\n",
            "Train Epoch: 131 [0/9013 (0%)]\tTraining Loss: 0.178545\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 131 [4096/9013 (45%)]\tTraining Loss: 0.287195\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 131 [8192/9013 (91%)]\tTraining Loss: 0.289472\tLR: 0.000625\n",
            "Testing: 131 [630/1002 (94%)]\tTesting Loss: 8.033e-02\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.495, Accuracy: 927/1002.000        (92.000%)\n",
            "Best Accuracy: 92.00%\n",
            "\n",
            "\n",
            "Train Epoch: 132 [0/9013 (0%)]\tTraining Loss: 0.178103\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 132 [4096/9013 (45%)]\tTraining Loss: 0.080786\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 132 [8192/9013 (91%)]\tTraining Loss: 0.332824\tLR: 0.000625\n",
            "Testing: 132 [630/1002 (94%)]\tTesting Loss: 2.491e-02\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.429, Accuracy: 924/1002.000        (92.000%)\n",
            "Best Accuracy: 92.00%\n",
            "\n",
            "\n",
            "Train Epoch: 133 [0/9013 (0%)]\tTraining Loss: 0.162857\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 133 [4096/9013 (45%)]\tTraining Loss: 0.110983\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 133 [8192/9013 (91%)]\tTraining Loss: 0.115443\tLR: 0.000625\n",
            "\n",
            "==> Saving model ...\n",
            "Testing: 133 [630/1002 (94%)]\tTesting Loss: 3.000e-01\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.483, Accuracy: 932/1002.000        (93.000%)\n",
            "Best Accuracy: 93.00%\n",
            "\n",
            "\n",
            "Train Epoch: 134 [0/9013 (0%)]\tTraining Loss: 0.159665\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 134 [4096/9013 (45%)]\tTraining Loss: 0.268370\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 134 [8192/9013 (91%)]\tTraining Loss: 0.365890\tLR: 0.000625\n",
            "Testing: 134 [630/1002 (94%)]\tTesting Loss: 1.273e+00\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.481, Accuracy: 923/1002.000        (92.000%)\n",
            "Best Accuracy: 93.00%\n",
            "\n",
            "\n",
            "Train Epoch: 135 [0/9013 (0%)]\tTraining Loss: 0.316943\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 135 [4096/9013 (45%)]\tTraining Loss: 0.229542\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 135 [8192/9013 (91%)]\tTraining Loss: 0.288119\tLR: 0.000625\n",
            "Testing: 135 [630/1002 (94%)]\tTesting Loss: 3.054e-01\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.496, Accuracy: 933/1002.000        (93.000%)\n",
            "Best Accuracy: 93.00%\n",
            "\n",
            "\n",
            "Train Epoch: 136 [0/9013 (0%)]\tTraining Loss: 0.247126\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 136 [4096/9013 (45%)]\tTraining Loss: 0.222056\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 136 [8192/9013 (91%)]\tTraining Loss: 0.109818\tLR: 0.000625\n",
            "Testing: 136 [630/1002 (94%)]\tTesting Loss: 7.332e-02\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.513, Accuracy: 930/1002.000        (92.000%)\n",
            "Best Accuracy: 93.00%\n",
            "\n",
            "\n",
            "Train Epoch: 137 [0/9013 (0%)]\tTraining Loss: 0.139862\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 137 [4096/9013 (45%)]\tTraining Loss: 0.558058\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 137 [8192/9013 (91%)]\tTraining Loss: 0.460946\tLR: 0.000625\n",
            "Testing: 137 [630/1002 (94%)]\tTesting Loss: 8.036e-01\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.643, Accuracy: 929/1002.000        (92.000%)\n",
            "Best Accuracy: 93.00%\n",
            "\n",
            "\n",
            "Train Epoch: 138 [0/9013 (0%)]\tTraining Loss: 0.174587\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 138 [4096/9013 (45%)]\tTraining Loss: 0.195240\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 138 [8192/9013 (91%)]\tTraining Loss: 0.279416\tLR: 0.000625\n",
            "Testing: 138 [630/1002 (94%)]\tTesting Loss: 3.517e-01\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.522, Accuracy: 933/1002.000        (93.000%)\n",
            "Best Accuracy: 93.00%\n",
            "\n",
            "\n",
            "Train Epoch: 139 [0/9013 (0%)]\tTraining Loss: 0.316896\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 139 [4096/9013 (45%)]\tTraining Loss: 0.332120\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 139 [8192/9013 (91%)]\tTraining Loss: 0.201789\tLR: 0.000625\n",
            "Testing: 139 [630/1002 (94%)]\tTesting Loss: 1.396e+00\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.723, Accuracy: 917/1002.000        (91.000%)\n",
            "Best Accuracy: 93.00%\n",
            "\n",
            "\n",
            "Train Epoch: 140 [0/9013 (0%)]\tTraining Loss: 0.167148\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 140 [4096/9013 (45%)]\tTraining Loss: 0.351986\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 140 [8192/9013 (91%)]\tTraining Loss: 0.236454\tLR: 0.000625\n",
            "Testing: 140 [630/1002 (94%)]\tTesting Loss: 3.597e-01\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.566, Accuracy: 936/1002.000        (93.000%)\n",
            "Best Accuracy: 93.00%\n",
            "\n",
            "\n",
            "Train Epoch: 141 [0/9013 (0%)]\tTraining Loss: 0.134225\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 141 [4096/9013 (45%)]\tTraining Loss: 0.195880\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 141 [8192/9013 (91%)]\tTraining Loss: 0.535141\tLR: 0.000625\n",
            "\n",
            "==> Saving model ...\n",
            "Testing: 141 [630/1002 (94%)]\tTesting Loss: 5.108e-01\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.595, Accuracy: 944/1002.000        (94.000%)\n",
            "Best Accuracy: 94.00%\n",
            "\n",
            "\n",
            "Train Epoch: 142 [0/9013 (0%)]\tTraining Loss: 0.246059\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 142 [4096/9013 (45%)]\tTraining Loss: 0.201497\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 142 [8192/9013 (91%)]\tTraining Loss: 0.415137\tLR: 0.000625\n",
            "Testing: 142 [630/1002 (94%)]\tTesting Loss: 9.815e-01\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.651, Accuracy: 939/1002.000        (93.000%)\n",
            "Best Accuracy: 94.00%\n",
            "\n",
            "\n",
            "Train Epoch: 143 [0/9013 (0%)]\tTraining Loss: 0.080037\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 143 [4096/9013 (45%)]\tTraining Loss: 0.078556\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 143 [8192/9013 (91%)]\tTraining Loss: 0.189278\tLR: 0.000625\n",
            "Testing: 143 [630/1002 (94%)]\tTesting Loss: 1.082e-01\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.608, Accuracy: 931/1002.000        (92.000%)\n",
            "Best Accuracy: 94.00%\n",
            "\n",
            "\n",
            "Train Epoch: 144 [0/9013 (0%)]\tTraining Loss: 0.114912\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 144 [4096/9013 (45%)]\tTraining Loss: 0.202559\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 144 [8192/9013 (91%)]\tTraining Loss: 0.145212\tLR: 0.000625\n",
            "Testing: 144 [630/1002 (94%)]\tTesting Loss: 2.342e+00\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.682, Accuracy: 933/1002.000        (93.000%)\n",
            "Best Accuracy: 94.00%\n",
            "\n",
            "\n",
            "Train Epoch: 145 [0/9013 (0%)]\tTraining Loss: 0.082464\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 145 [4096/9013 (45%)]\tTraining Loss: 0.358817\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 145 [8192/9013 (91%)]\tTraining Loss: 0.239158\tLR: 0.000625\n",
            "Testing: 145 [630/1002 (94%)]\tTesting Loss: 1.779e+00\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.686, Accuracy: 933/1002.000        (93.000%)\n",
            "Best Accuracy: 94.00%\n",
            "\n",
            "\n",
            "Train Epoch: 146 [0/9013 (0%)]\tTraining Loss: 0.139108\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 146 [4096/9013 (45%)]\tTraining Loss: 0.173851\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 146 [8192/9013 (91%)]\tTraining Loss: 0.137284\tLR: 0.000625\n",
            "Testing: 146 [630/1002 (94%)]\tTesting Loss: 2.512e+00\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.799, Accuracy: 936/1002.000        (93.000%)\n",
            "Best Accuracy: 94.00%\n",
            "\n",
            "\n",
            "Train Epoch: 147 [0/9013 (0%)]\tTraining Loss: 0.123228\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 147 [4096/9013 (45%)]\tTraining Loss: 0.168508\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 147 [8192/9013 (91%)]\tTraining Loss: 0.042511\tLR: 0.000625\n",
            "Testing: 147 [630/1002 (94%)]\tTesting Loss: 1.060e+00\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.685, Accuracy: 931/1002.000        (92.000%)\n",
            "Best Accuracy: 94.00%\n",
            "\n",
            "\n",
            "Train Epoch: 148 [0/9013 (0%)]\tTraining Loss: 0.330247\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 148 [4096/9013 (45%)]\tTraining Loss: 0.232042\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 148 [8192/9013 (91%)]\tTraining Loss: 0.226175\tLR: 0.000625\n",
            "Testing: 148 [630/1002 (94%)]\tTesting Loss: 4.828e-01\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.673, Accuracy: 936/1002.000        (93.000%)\n",
            "Best Accuracy: 94.00%\n",
            "\n",
            "\n",
            "Train Epoch: 149 [0/9013 (0%)]\tTraining Loss: 0.303861\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 149 [4096/9013 (45%)]\tTraining Loss: 0.392093\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 149 [8192/9013 (91%)]\tTraining Loss: 0.192684\tLR: 0.000625\n",
            "Testing: 149 [630/1002 (94%)]\tTesting Loss: 1.357e-01\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.832, Accuracy: 928/1002.000        (92.000%)\n",
            "Best Accuracy: 94.00%\n",
            "\n",
            "\n",
            "Train Epoch: 150 [0/9013 (0%)]\tTraining Loss: 0.083663\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 150 [4096/9013 (45%)]\tTraining Loss: 0.313595\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 150 [8192/9013 (91%)]\tTraining Loss: 0.216917\tLR: 0.000625\n",
            "Testing: 150 [630/1002 (94%)]\tTesting Loss: 1.856e+00\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.732, Accuracy: 938/1002.000        (93.000%)\n",
            "Best Accuracy: 94.00%\n",
            "\n",
            "\n",
            "Train Epoch: 151 [0/9013 (0%)]\tTraining Loss: 0.284281\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 151 [4096/9013 (45%)]\tTraining Loss: 0.085212\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 151 [8192/9013 (91%)]\tTraining Loss: 0.235584\tLR: 0.000625\n",
            "Testing: 151 [630/1002 (94%)]\tTesting Loss: 1.833e-01\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.803, Accuracy: 931/1002.000        (92.000%)\n",
            "Best Accuracy: 94.00%\n",
            "\n",
            "\n",
            "Train Epoch: 152 [0/9013 (0%)]\tTraining Loss: 0.219192\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 152 [4096/9013 (45%)]\tTraining Loss: 0.143758\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 152 [8192/9013 (91%)]\tTraining Loss: 0.326439\tLR: 0.000625\n",
            "Testing: 152 [630/1002 (94%)]\tTesting Loss: 5.474e-01\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.792, Accuracy: 943/1002.000        (94.000%)\n",
            "Best Accuracy: 94.00%\n",
            "\n",
            "\n",
            "Train Epoch: 153 [0/9013 (0%)]\tTraining Loss: 0.146166\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 153 [4096/9013 (45%)]\tTraining Loss: 0.213036\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 153 [8192/9013 (91%)]\tTraining Loss: 0.181310\tLR: 0.000625\n",
            "Testing: 153 [630/1002 (94%)]\tTesting Loss: 6.497e-02\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.842, Accuracy: 919/1002.000        (91.000%)\n",
            "Best Accuracy: 94.00%\n",
            "\n",
            "\n",
            "Train Epoch: 154 [0/9013 (0%)]\tTraining Loss: 0.254936\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 154 [4096/9013 (45%)]\tTraining Loss: 0.256394\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 154 [8192/9013 (91%)]\tTraining Loss: 0.282298\tLR: 0.000625\n",
            "Testing: 154 [630/1002 (94%)]\tTesting Loss: 5.231e-01\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.768, Accuracy: 942/1002.000        (94.000%)\n",
            "Best Accuracy: 94.00%\n",
            "\n",
            "\n",
            "Train Epoch: 155 [0/9013 (0%)]\tTraining Loss: 0.136617\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 155 [4096/9013 (45%)]\tTraining Loss: 0.087213\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 155 [8192/9013 (91%)]\tTraining Loss: 0.027414\tLR: 0.000625\n",
            "Testing: 155 [630/1002 (94%)]\tTesting Loss: 1.515e+00\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.993, Accuracy: 934/1002.000        (93.000%)\n",
            "Best Accuracy: 94.00%\n",
            "\n",
            "\n",
            "Train Epoch: 156 [0/9013 (0%)]\tTraining Loss: 0.033206\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 156 [4096/9013 (45%)]\tTraining Loss: 0.060620\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 156 [8192/9013 (91%)]\tTraining Loss: 0.189411\tLR: 0.000625\n",
            "Testing: 156 [630/1002 (94%)]\tTesting Loss: 1.909e+00\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 2.097, Accuracy: 937/1002.000        (93.000%)\n",
            "Best Accuracy: 94.00%\n",
            "\n",
            "\n",
            "Train Epoch: 157 [0/9013 (0%)]\tTraining Loss: 0.088677\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 157 [4096/9013 (45%)]\tTraining Loss: 0.231509\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 157 [8192/9013 (91%)]\tTraining Loss: 0.173785\tLR: 0.000625\n",
            "Testing: 157 [630/1002 (94%)]\tTesting Loss: 1.554e-02\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.850, Accuracy: 930/1002.000        (92.000%)\n",
            "Best Accuracy: 94.00%\n",
            "\n",
            "\n",
            "Train Epoch: 158 [0/9013 (0%)]\tTraining Loss: 0.091694\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 158 [4096/9013 (45%)]\tTraining Loss: 0.079006\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 158 [8192/9013 (91%)]\tTraining Loss: 0.046694\tLR: 0.000625\n",
            "Testing: 158 [630/1002 (94%)]\tTesting Loss: 8.258e-01\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 2.028, Accuracy: 931/1002.000        (92.000%)\n",
            "Best Accuracy: 94.00%\n",
            "\n",
            "\n",
            "Train Epoch: 159 [0/9013 (0%)]\tTraining Loss: 0.227674\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 159 [4096/9013 (45%)]\tTraining Loss: 0.235923\tLR: 0.000625\n",
            "\n",
            "Train Epoch: 159 [8192/9013 (91%)]\tTraining Loss: 0.152816\tLR: 0.000625\n",
            "Testing: 159 [630/1002 (94%)]\tTesting Loss: 1.214e+00\tLR: 0.000625\n",
            "\n",
            "Test set: Average loss: 1.938, Accuracy: 933/1002.000        (93.000%)\n",
            "Best Accuracy: 94.00%\n",
            "\n",
            "\n",
            "Train Epoch: 160 [0/9013 (0%)]\tTraining Loss: 0.278624\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 160 [4096/9013 (45%)]\tTraining Loss: 0.212072\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 160 [8192/9013 (91%)]\tTraining Loss: 0.083195\tLR: 0.0003125\n",
            "Testing: 160 [630/1002 (94%)]\tTesting Loss: 7.323e-01\tLR: 0.0003125\n",
            "\n",
            "Test set: Average loss: 1.930, Accuracy: 947/1002.000        (94.000%)\n",
            "Best Accuracy: 94.00%\n",
            "\n",
            "\n",
            "Train Epoch: 161 [0/9013 (0%)]\tTraining Loss: 0.076164\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 161 [4096/9013 (45%)]\tTraining Loss: 0.189906\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 161 [8192/9013 (91%)]\tTraining Loss: 0.074605\tLR: 0.0003125\n",
            "Testing: 161 [630/1002 (94%)]\tTesting Loss: 7.910e-01\tLR: 0.0003125\n",
            "\n",
            "Test set: Average loss: 1.948, Accuracy: 947/1002.000        (94.000%)\n",
            "Best Accuracy: 94.00%\n",
            "\n",
            "\n",
            "Train Epoch: 162 [0/9013 (0%)]\tTraining Loss: 0.137937\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 162 [4096/9013 (45%)]\tTraining Loss: 0.071435\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 162 [8192/9013 (91%)]\tTraining Loss: 0.056592\tLR: 0.0003125\n",
            "Testing: 162 [630/1002 (94%)]\tTesting Loss: 2.600e+00\tLR: 0.0003125\n",
            "\n",
            "Test set: Average loss: 2.027, Accuracy: 945/1002.000        (94.000%)\n",
            "Best Accuracy: 94.00%\n",
            "\n",
            "\n",
            "Train Epoch: 163 [0/9013 (0%)]\tTraining Loss: 0.055634\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 163 [4096/9013 (45%)]\tTraining Loss: 0.096510\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 163 [8192/9013 (91%)]\tTraining Loss: 0.123022\tLR: 0.0003125\n",
            "Testing: 163 [630/1002 (94%)]\tTesting Loss: 3.806e-01\tLR: 0.0003125\n",
            "\n",
            "Test set: Average loss: 1.984, Accuracy: 950/1002.000        (94.000%)\n",
            "Best Accuracy: 94.00%\n",
            "\n",
            "\n",
            "Train Epoch: 164 [0/9013 (0%)]\tTraining Loss: 0.062608\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 164 [4096/9013 (45%)]\tTraining Loss: 0.038126\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 164 [8192/9013 (91%)]\tTraining Loss: 0.293317\tLR: 0.0003125\n",
            "Testing: 164 [630/1002 (94%)]\tTesting Loss: 1.369e+00\tLR: 0.0003125\n",
            "\n",
            "Test set: Average loss: 2.072, Accuracy: 951/1002.000        (94.000%)\n",
            "Best Accuracy: 94.00%\n",
            "\n",
            "\n",
            "Train Epoch: 165 [0/9013 (0%)]\tTraining Loss: 0.141622\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 165 [4096/9013 (45%)]\tTraining Loss: 0.143416\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 165 [8192/9013 (91%)]\tTraining Loss: 0.103213\tLR: 0.0003125\n",
            "\n",
            "==> Saving model ...\n",
            "Testing: 165 [630/1002 (94%)]\tTesting Loss: 1.110e+00\tLR: 0.0003125\n",
            "\n",
            "Test set: Average loss: 2.062, Accuracy: 953/1002.000        (95.000%)\n",
            "Best Accuracy: 95.00%\n",
            "\n",
            "\n",
            "Train Epoch: 166 [0/9013 (0%)]\tTraining Loss: 0.219170\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 166 [4096/9013 (45%)]\tTraining Loss: 0.110834\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 166 [8192/9013 (91%)]\tTraining Loss: 0.117403\tLR: 0.0003125\n",
            "Testing: 166 [630/1002 (94%)]\tTesting Loss: 8.570e-03\tLR: 0.0003125\n",
            "\n",
            "Test set: Average loss: 2.088, Accuracy: 950/1002.000        (94.000%)\n",
            "Best Accuracy: 95.00%\n",
            "\n",
            "\n",
            "Train Epoch: 167 [0/9013 (0%)]\tTraining Loss: 0.021747\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 167 [4096/9013 (45%)]\tTraining Loss: 0.163389\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 167 [8192/9013 (91%)]\tTraining Loss: 0.009484\tLR: 0.0003125\n",
            "Testing: 167 [630/1002 (94%)]\tTesting Loss: 2.029e+00\tLR: 0.0003125\n",
            "\n",
            "Test set: Average loss: 2.189, Accuracy: 951/1002.000        (94.000%)\n",
            "Best Accuracy: 95.00%\n",
            "\n",
            "\n",
            "Train Epoch: 168 [0/9013 (0%)]\tTraining Loss: 0.240230\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 168 [4096/9013 (45%)]\tTraining Loss: 0.028639\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 168 [8192/9013 (91%)]\tTraining Loss: 0.057365\tLR: 0.0003125\n",
            "Testing: 168 [630/1002 (94%)]\tTesting Loss: 1.214e-01\tLR: 0.0003125\n",
            "\n",
            "Test set: Average loss: 2.098, Accuracy: 954/1002.000        (95.000%)\n",
            "Best Accuracy: 95.00%\n",
            "\n",
            "\n",
            "Train Epoch: 169 [0/9013 (0%)]\tTraining Loss: 0.066864\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 169 [4096/9013 (45%)]\tTraining Loss: 0.157463\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 169 [8192/9013 (91%)]\tTraining Loss: 0.115435\tLR: 0.0003125\n",
            "Testing: 169 [630/1002 (94%)]\tTesting Loss: 5.071e-01\tLR: 0.0003125\n",
            "\n",
            "Test set: Average loss: 2.124, Accuracy: 953/1002.000        (95.000%)\n",
            "Best Accuracy: 95.00%\n",
            "\n",
            "\n",
            "Train Epoch: 170 [0/9013 (0%)]\tTraining Loss: 0.354348\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 170 [4096/9013 (45%)]\tTraining Loss: 0.058834\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 170 [8192/9013 (91%)]\tTraining Loss: 0.338935\tLR: 0.0003125\n",
            "Testing: 170 [630/1002 (94%)]\tTesting Loss: 1.509e-01\tLR: 0.0003125\n",
            "\n",
            "Test set: Average loss: 2.163, Accuracy: 949/1002.000        (94.000%)\n",
            "Best Accuracy: 95.00%\n",
            "\n",
            "\n",
            "Train Epoch: 171 [0/9013 (0%)]\tTraining Loss: 0.020663\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 171 [4096/9013 (45%)]\tTraining Loss: 0.018065\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 171 [8192/9013 (91%)]\tTraining Loss: 0.090632\tLR: 0.0003125\n",
            "Testing: 171 [630/1002 (94%)]\tTesting Loss: 1.068e+00\tLR: 0.0003125\n",
            "\n",
            "Test set: Average loss: 2.260, Accuracy: 953/1002.000        (95.000%)\n",
            "Best Accuracy: 95.00%\n",
            "\n",
            "\n",
            "Train Epoch: 172 [0/9013 (0%)]\tTraining Loss: 0.011509\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 172 [4096/9013 (45%)]\tTraining Loss: 0.103977\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 172 [8192/9013 (91%)]\tTraining Loss: 0.044548\tLR: 0.0003125\n",
            "Testing: 172 [630/1002 (94%)]\tTesting Loss: 1.966e+00\tLR: 0.0003125\n",
            "\n",
            "Test set: Average loss: 2.408, Accuracy: 953/1002.000        (95.000%)\n",
            "Best Accuracy: 95.00%\n",
            "\n",
            "\n",
            "Train Epoch: 173 [0/9013 (0%)]\tTraining Loss: 0.090739\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 173 [4096/9013 (45%)]\tTraining Loss: 0.196885\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 173 [8192/9013 (91%)]\tTraining Loss: 0.182696\tLR: 0.0003125\n",
            "Testing: 173 [630/1002 (94%)]\tTesting Loss: 8.568e-01\tLR: 0.0003125\n",
            "\n",
            "Test set: Average loss: 2.375, Accuracy: 951/1002.000        (94.000%)\n",
            "Best Accuracy: 95.00%\n",
            "\n",
            "\n",
            "Train Epoch: 174 [0/9013 (0%)]\tTraining Loss: 0.028173\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 174 [4096/9013 (45%)]\tTraining Loss: 0.109784\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 174 [8192/9013 (91%)]\tTraining Loss: 0.181219\tLR: 0.0003125\n",
            "Testing: 174 [630/1002 (94%)]\tTesting Loss: 3.413e-02\tLR: 0.0003125\n",
            "\n",
            "Test set: Average loss: 2.327, Accuracy: 950/1002.000        (94.000%)\n",
            "Best Accuracy: 95.00%\n",
            "\n",
            "\n",
            "Train Epoch: 175 [0/9013 (0%)]\tTraining Loss: 0.091295\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 175 [4096/9013 (45%)]\tTraining Loss: 0.185329\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 175 [8192/9013 (91%)]\tTraining Loss: 0.040956\tLR: 0.0003125\n",
            "Testing: 175 [630/1002 (94%)]\tTesting Loss: 7.383e-01\tLR: 0.0003125\n",
            "\n",
            "Test set: Average loss: 2.311, Accuracy: 953/1002.000        (95.000%)\n",
            "Best Accuracy: 95.00%\n",
            "\n",
            "\n",
            "Train Epoch: 176 [0/9013 (0%)]\tTraining Loss: 0.181070\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 176 [4096/9013 (45%)]\tTraining Loss: 0.025258\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 176 [8192/9013 (91%)]\tTraining Loss: 0.024827\tLR: 0.0003125\n",
            "Testing: 176 [630/1002 (94%)]\tTesting Loss: 1.693e+00\tLR: 0.0003125\n",
            "\n",
            "Test set: Average loss: 2.477, Accuracy: 953/1002.000        (95.000%)\n",
            "Best Accuracy: 95.00%\n",
            "\n",
            "\n",
            "Train Epoch: 177 [0/9013 (0%)]\tTraining Loss: 0.075547\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 177 [4096/9013 (45%)]\tTraining Loss: 0.124149\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 177 [8192/9013 (91%)]\tTraining Loss: 0.197184\tLR: 0.0003125\n",
            "Testing: 177 [630/1002 (94%)]\tTesting Loss: 2.317e-02\tLR: 0.0003125\n",
            "\n",
            "Test set: Average loss: 2.441, Accuracy: 945/1002.000        (94.000%)\n",
            "Best Accuracy: 95.00%\n",
            "\n",
            "\n",
            "Train Epoch: 178 [0/9013 (0%)]\tTraining Loss: 0.486048\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 178 [4096/9013 (45%)]\tTraining Loss: 0.034235\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 178 [8192/9013 (91%)]\tTraining Loss: 0.150129\tLR: 0.0003125\n",
            "Testing: 178 [630/1002 (94%)]\tTesting Loss: 1.525e+00\tLR: 0.0003125\n",
            "\n",
            "Test set: Average loss: 2.455, Accuracy: 950/1002.000        (94.000%)\n",
            "Best Accuracy: 95.00%\n",
            "\n",
            "\n",
            "Train Epoch: 179 [0/9013 (0%)]\tTraining Loss: 0.022738\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 179 [4096/9013 (45%)]\tTraining Loss: 0.177077\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 179 [8192/9013 (91%)]\tTraining Loss: 0.244267\tLR: 0.0003125\n",
            "Testing: 179 [630/1002 (94%)]\tTesting Loss: 5.991e-01\tLR: 0.0003125\n",
            "\n",
            "Test set: Average loss: 2.428, Accuracy: 950/1002.000        (94.000%)\n",
            "Best Accuracy: 95.00%\n",
            "\n",
            "\n",
            "Train Epoch: 180 [0/9013 (0%)]\tTraining Loss: 0.138751\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 180 [4096/9013 (45%)]\tTraining Loss: 0.163313\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 180 [8192/9013 (91%)]\tTraining Loss: 0.288600\tLR: 0.0003125\n",
            "Testing: 180 [630/1002 (94%)]\tTesting Loss: 6.403e-01\tLR: 0.0003125\n",
            "\n",
            "Test set: Average loss: 2.332, Accuracy: 950/1002.000        (94.000%)\n",
            "Best Accuracy: 95.00%\n",
            "\n",
            "\n",
            "Train Epoch: 181 [0/9013 (0%)]\tTraining Loss: 0.100546\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 181 [4096/9013 (45%)]\tTraining Loss: 0.047616\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 181 [8192/9013 (91%)]\tTraining Loss: 0.090071\tLR: 0.0003125\n",
            "Testing: 181 [630/1002 (94%)]\tTesting Loss: 2.441e+00\tLR: 0.0003125\n",
            "\n",
            "Test set: Average loss: 2.312, Accuracy: 954/1002.000        (95.000%)\n",
            "Best Accuracy: 95.00%\n",
            "\n",
            "\n",
            "Train Epoch: 182 [0/9013 (0%)]\tTraining Loss: 0.015899\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 182 [4096/9013 (45%)]\tTraining Loss: 0.115706\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 182 [8192/9013 (91%)]\tTraining Loss: 0.134455\tLR: 0.0003125\n",
            "Testing: 182 [630/1002 (94%)]\tTesting Loss: 4.266e+00\tLR: 0.0003125\n",
            "\n",
            "Test set: Average loss: 2.494, Accuracy: 956/1002.000        (95.000%)\n",
            "Best Accuracy: 95.00%\n",
            "\n",
            "\n",
            "Train Epoch: 183 [0/9013 (0%)]\tTraining Loss: 0.086613\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 183 [4096/9013 (45%)]\tTraining Loss: 0.048685\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 183 [8192/9013 (91%)]\tTraining Loss: 0.024182\tLR: 0.0003125\n",
            "Testing: 183 [630/1002 (94%)]\tTesting Loss: 6.712e-03\tLR: 0.0003125\n",
            "\n",
            "Test set: Average loss: 2.407, Accuracy: 956/1002.000        (95.000%)\n",
            "Best Accuracy: 95.00%\n",
            "\n",
            "\n",
            "Train Epoch: 184 [0/9013 (0%)]\tTraining Loss: 0.125962\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 184 [4096/9013 (45%)]\tTraining Loss: 0.063820\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 184 [8192/9013 (91%)]\tTraining Loss: 0.063870\tLR: 0.0003125\n",
            "Testing: 184 [630/1002 (94%)]\tTesting Loss: 3.909e+00\tLR: 0.0003125\n",
            "\n",
            "Test set: Average loss: 2.690, Accuracy: 954/1002.000        (95.000%)\n",
            "Best Accuracy: 95.00%\n",
            "\n",
            "\n",
            "Train Epoch: 185 [0/9013 (0%)]\tTraining Loss: 0.103581\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 185 [4096/9013 (45%)]\tTraining Loss: 0.017458\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 185 [8192/9013 (91%)]\tTraining Loss: 0.191722\tLR: 0.0003125\n",
            "Testing: 185 [630/1002 (94%)]\tTesting Loss: 8.719e-03\tLR: 0.0003125\n",
            "\n",
            "Test set: Average loss: 2.559, Accuracy: 956/1002.000        (95.000%)\n",
            "Best Accuracy: 95.00%\n",
            "\n",
            "\n",
            "Train Epoch: 186 [0/9013 (0%)]\tTraining Loss: 0.086438\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 186 [4096/9013 (45%)]\tTraining Loss: 0.157537\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 186 [8192/9013 (91%)]\tTraining Loss: 0.045863\tLR: 0.0003125\n",
            "Testing: 186 [630/1002 (94%)]\tTesting Loss: 1.371e-01\tLR: 0.0003125\n",
            "\n",
            "Test set: Average loss: 2.537, Accuracy: 953/1002.000        (95.000%)\n",
            "Best Accuracy: 95.00%\n",
            "\n",
            "\n",
            "Train Epoch: 187 [0/9013 (0%)]\tTraining Loss: 0.046283\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 187 [4096/9013 (45%)]\tTraining Loss: 0.065154\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 187 [8192/9013 (91%)]\tTraining Loss: 0.285696\tLR: 0.0003125\n",
            "Testing: 187 [630/1002 (94%)]\tTesting Loss: 5.732e-02\tLR: 0.0003125\n",
            "\n",
            "Test set: Average loss: 2.635, Accuracy: 953/1002.000        (95.000%)\n",
            "Best Accuracy: 95.00%\n",
            "\n",
            "\n",
            "Train Epoch: 188 [0/9013 (0%)]\tTraining Loss: 0.024704\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 188 [4096/9013 (45%)]\tTraining Loss: 0.037158\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 188 [8192/9013 (91%)]\tTraining Loss: 0.070064\tLR: 0.0003125\n",
            "Testing: 188 [630/1002 (94%)]\tTesting Loss: 1.857e+00\tLR: 0.0003125\n",
            "\n",
            "Test set: Average loss: 2.849, Accuracy: 957/1002.000        (95.000%)\n",
            "Best Accuracy: 95.00%\n",
            "\n",
            "\n",
            "Train Epoch: 189 [0/9013 (0%)]\tTraining Loss: 0.076643\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 189 [4096/9013 (45%)]\tTraining Loss: 0.013339\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 189 [8192/9013 (91%)]\tTraining Loss: 0.054423\tLR: 0.0003125\n",
            "Testing: 189 [630/1002 (94%)]\tTesting Loss: 2.988e+00\tLR: 0.0003125\n",
            "\n",
            "Test set: Average loss: 2.868, Accuracy: 957/1002.000        (95.000%)\n",
            "Best Accuracy: 95.00%\n",
            "\n",
            "\n",
            "Train Epoch: 190 [0/9013 (0%)]\tTraining Loss: 0.220902\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 190 [4096/9013 (45%)]\tTraining Loss: 0.038252\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 190 [8192/9013 (91%)]\tTraining Loss: 0.106062\tLR: 0.0003125\n",
            "Testing: 190 [630/1002 (94%)]\tTesting Loss: 4.589e-01\tLR: 0.0003125\n",
            "\n",
            "Test set: Average loss: 2.636, Accuracy: 954/1002.000        (95.000%)\n",
            "Best Accuracy: 95.00%\n",
            "\n",
            "\n",
            "Train Epoch: 191 [0/9013 (0%)]\tTraining Loss: 0.071716\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 191 [4096/9013 (45%)]\tTraining Loss: 0.072440\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 191 [8192/9013 (91%)]\tTraining Loss: 0.006617\tLR: 0.0003125\n",
            "Testing: 191 [630/1002 (94%)]\tTesting Loss: 1.726e+00\tLR: 0.0003125\n",
            "\n",
            "Test set: Average loss: 2.843, Accuracy: 955/1002.000        (95.000%)\n",
            "Best Accuracy: 95.00%\n",
            "\n",
            "\n",
            "Train Epoch: 192 [0/9013 (0%)]\tTraining Loss: 0.043601\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 192 [4096/9013 (45%)]\tTraining Loss: 0.132236\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 192 [8192/9013 (91%)]\tTraining Loss: 0.182845\tLR: 0.0003125\n",
            "Testing: 192 [630/1002 (94%)]\tTesting Loss: 4.041e+00\tLR: 0.0003125\n",
            "\n",
            "Test set: Average loss: 2.861, Accuracy: 953/1002.000        (95.000%)\n",
            "Best Accuracy: 95.00%\n",
            "\n",
            "\n",
            "Train Epoch: 193 [0/9013 (0%)]\tTraining Loss: 0.081087\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 193 [4096/9013 (45%)]\tTraining Loss: 0.074423\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 193 [8192/9013 (91%)]\tTraining Loss: 0.192435\tLR: 0.0003125\n",
            "Testing: 193 [630/1002 (94%)]\tTesting Loss: 2.833e-01\tLR: 0.0003125\n",
            "\n",
            "Test set: Average loss: 2.727, Accuracy: 948/1002.000        (94.000%)\n",
            "Best Accuracy: 95.00%\n",
            "\n",
            "\n",
            "Train Epoch: 194 [0/9013 (0%)]\tTraining Loss: 0.098641\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 194 [4096/9013 (45%)]\tTraining Loss: 0.115977\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 194 [8192/9013 (91%)]\tTraining Loss: 0.143801\tLR: 0.0003125\n",
            "Testing: 194 [630/1002 (94%)]\tTesting Loss: 2.639e+00\tLR: 0.0003125\n",
            "\n",
            "Test set: Average loss: 2.905, Accuracy: 953/1002.000        (95.000%)\n",
            "Best Accuracy: 95.00%\n",
            "\n",
            "\n",
            "Train Epoch: 195 [0/9013 (0%)]\tTraining Loss: 0.019539\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 195 [4096/9013 (45%)]\tTraining Loss: 0.048819\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 195 [8192/9013 (91%)]\tTraining Loss: 0.158772\tLR: 0.0003125\n",
            "Testing: 195 [630/1002 (94%)]\tTesting Loss: 1.513e-01\tLR: 0.0003125\n",
            "\n",
            "Test set: Average loss: 2.859, Accuracy: 957/1002.000        (95.000%)\n",
            "Best Accuracy: 95.00%\n",
            "\n",
            "\n",
            "Train Epoch: 196 [0/9013 (0%)]\tTraining Loss: 0.077829\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 196 [4096/9013 (45%)]\tTraining Loss: 0.066771\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 196 [8192/9013 (91%)]\tTraining Loss: 0.024128\tLR: 0.0003125\n",
            "Testing: 196 [630/1002 (94%)]\tTesting Loss: 1.485e+00\tLR: 0.0003125\n",
            "\n",
            "Test set: Average loss: 2.898, Accuracy: 954/1002.000        (95.000%)\n",
            "Best Accuracy: 95.00%\n",
            "\n",
            "\n",
            "Train Epoch: 197 [0/9013 (0%)]\tTraining Loss: 0.079091\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 197 [4096/9013 (45%)]\tTraining Loss: 0.025815\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 197 [8192/9013 (91%)]\tTraining Loss: 0.149961\tLR: 0.0003125\n",
            "Testing: 197 [630/1002 (94%)]\tTesting Loss: 3.627e+00\tLR: 0.0003125\n",
            "\n",
            "Test set: Average loss: 3.011, Accuracy: 956/1002.000        (95.000%)\n",
            "Best Accuracy: 95.00%\n",
            "\n",
            "\n",
            "Train Epoch: 198 [0/9013 (0%)]\tTraining Loss: 0.005503\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 198 [4096/9013 (45%)]\tTraining Loss: 0.023958\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 198 [8192/9013 (91%)]\tTraining Loss: 0.168860\tLR: 0.0003125\n",
            "Testing: 198 [630/1002 (94%)]\tTesting Loss: 1.494e+00\tLR: 0.0003125\n",
            "\n",
            "Test set: Average loss: 2.767, Accuracy: 953/1002.000        (95.000%)\n",
            "Best Accuracy: 95.00%\n",
            "\n",
            "\n",
            "Train Epoch: 199 [0/9013 (0%)]\tTraining Loss: 0.004421\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 199 [4096/9013 (45%)]\tTraining Loss: 0.005452\tLR: 0.0003125\n",
            "\n",
            "Train Epoch: 199 [8192/9013 (91%)]\tTraining Loss: 0.066018\tLR: 0.0003125\n",
            "Testing: 199 [630/1002 (94%)]\tTesting Loss: 6.831e-03\tLR: 0.0003125\n",
            "\n",
            "Test set: Average loss: 2.816, Accuracy: 958/1002.000        (95.000%)\n",
            "Best Accuracy: 95.00%\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Epi09tTwKEZb",
        "colab_type": "code",
        "outputId": "111f3ff3-d538-4b39-df2f-14b411607fbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "cell_type": "code",
      "source": [
        "plt.plot(test_losses, label = \"Test Loss\")\n",
        "plt.plot(train_losses, label = \"Train Loss\")\n",
        "plt.legend()"
      ],
      "execution_count": 273,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f9157558400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 273
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFKCAYAAAA0WNeQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyhsAAMNQ3gAAGIbyBgDAMP8f5zqVoHv7/SIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f9158817208>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "sSoNQVmxGZ-u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2AfwSTVJK_uh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "img ="
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}